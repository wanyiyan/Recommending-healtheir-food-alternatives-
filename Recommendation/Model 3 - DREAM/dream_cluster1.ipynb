{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to create a recommendation engine (DREAM) for cluster 1 from customer segmentation of instacart dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "   * Input: sequence of baskets of cluster 1 customers\n",
    "   * Negative sample : The food items which is never purchased by customers\n",
    "   \n",
    "### Evaluation Metric (Top 15 items)\n",
    "   * Hit Rate @15\n",
    "       - Counts the fraction of times that the ground truth next item is among the top 15 items.\n",
    "       - we only have one test item for each user, Hit@15 is equivalent to Recall@15\n",
    "       - It is also propotional to Precision@15\n",
    "   * NDCG@15\n",
    "       - A position aware metric with assigns larger weights on higher positions.\n",
    "       \n",
    "### Model \n",
    "   * Model is saved under runs folder with this key 1605813697\n",
    "   * This key 1605813697 is required to run it on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import data_helper as dh\n",
    "from configc2 import Config\n",
    "from rnn_model import DRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"✔︎ DREAM Model Training...\")\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/training-{0}.log\".format(time.asctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:torch-log:                                         MODEL_DIR|runs/                                             \n",
      "INFO:torch-log:                                       NEG_SAMPLES|../data/neg_sample_insta_all_sampled_b4.pickle    \n",
      "INFO:torch-log:                                       TESTSET_DIR|../data/allb4_cluster1_test.json                  \n",
      "INFO:torch-log:                                   TRAININGSET_DIR|../data/allb4_cluster1_train.json                 \n",
      "INFO:torch-log:                                 VALIDATIONSET_DIR|../data/allb4_cluster1_val.json                   \n",
      "INFO:torch-log:                                  BASKET_POOL_TYPE|max                                               \n",
      "INFO:torch-log:                                        BATCH_SIZE|500                                               \n",
      "INFO:torch-log:                                              CLIP|10                                                \n",
      "INFO:torch-log:                                              CUDA|1                                                 \n",
      "INFO:torch-log:                                           DROPOUT|0.5                                               \n",
      "INFO:torch-log:                                     EMBEDDING_DIM|32                                                \n",
      "INFO:torch-log:                                            EPOCHS|111                                               \n",
      "INFO:torch-log:                                     LEARNING_RATE|0.01                                              \n",
      "INFO:torch-log:                                      LOG_INTERVAL|1                                                 \n",
      "INFO:torch-log:                                           NEG_NUM|500                                               \n",
      "INFO:torch-log:                                       NUM_PRODUCT|58925                                             \n",
      "INFO:torch-log:                                     RNN_LAYER_NUM|2                                                 \n",
      "INFO:torch-log:                                          RNN_TYPE|LSTM                                              \n",
      "INFO:torch-log:                                           SEQ_LEN|99                                                \n",
      "INFO:torch-log:                                             TOP_K|15                                                \n",
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dilim = '-' * 120\n",
    "logger.info(dilim)\n",
    "for attr in sorted(Config().__dict__):\n",
    "    logger.info('{:>50}|{:<50}'.format(attr.upper(), Config().__dict__[attr]))\n",
    "logger.info(dilim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Validation data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n",
      "INFO:torch-log:Save into /home/reshmask/Next-Basket-Recommendation-master/DREAM/runs/1606163676\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:82: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     1 /    47 | ms/batch 751.75 | Loss 1.4692 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     2 /    47 | ms/batch 390.50 | Loss 0.6960 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     3 /    47 | ms/batch 388.84 | Loss 0.6902 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     4 /    47 | ms/batch 390.76 | Loss 0.6799 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     5 /    47 | ms/batch 415.52 | Loss 0.6732 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     6 /    47 | ms/batch 324.05 | Loss 0.6665 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     7 /    47 | ms/batch 366.36 | Loss 0.6610 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     8 /    47 | ms/batch 384.03 | Loss 0.6510 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     9 /    47 | ms/batch 376.89 | Loss 0.6472 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    10 /    47 | ms/batch 396.69 | Loss 0.6404 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    11 /    47 | ms/batch 376.46 | Loss 0.6288 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    12 /    47 | ms/batch 389.91 | Loss 0.6208 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    13 /    47 | ms/batch 316.84 | Loss 0.6037 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    14 /    47 | ms/batch 393.33 | Loss 0.5988 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    15 /    47 | ms/batch 400.97 | Loss 0.5858 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    16 /    47 | ms/batch 389.32 | Loss 0.5770 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    17 /    47 | ms/batch 381.18 | Loss 0.5605 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    18 /    47 | ms/batch 398.24 | Loss 0.5446 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    19 /    47 | ms/batch 409.58 | Loss 0.5345 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    20 /    47 | ms/batch 406.13 | Loss 0.5117 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    21 /    47 | ms/batch 393.11 | Loss 0.5067 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    22 /    47 | ms/batch 406.61 | Loss 0.4878 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    23 /    47 | ms/batch 381.92 | Loss 0.4680 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    24 /    47 | ms/batch 382.34 | Loss 0.4617 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    25 /    47 | ms/batch 403.37 | Loss 0.4516 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    26 /    47 | ms/batch 376.03 | Loss 0.4234 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    27 /    47 | ms/batch 422.34 | Loss 0.4094 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    28 /    47 | ms/batch 419.21 | Loss 0.4001 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    29 /    47 | ms/batch 398.27 | Loss 0.4035 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    30 /    47 | ms/batch 401.88 | Loss 0.3712 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    31 /    47 | ms/batch 354.21 | Loss 0.3584 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    32 /    47 | ms/batch 323.79 | Loss 0.3653 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    33 /    47 | ms/batch 384.30 | Loss 0.3457 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    34 /    47 | ms/batch 367.33 | Loss 0.3421 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    35 /    47 | ms/batch 371.79 | Loss 0.3346 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    36 /    47 | ms/batch 410.87 | Loss 0.3114 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    37 /    47 | ms/batch 346.46 | Loss 0.2984 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    38 /    47 | ms/batch 408.99 | Loss 0.2928 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    39 /    47 | ms/batch 408.02 | Loss 0.2837 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    40 /    47 | ms/batch 387.26 | Loss 0.2807 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    41 /    47 | ms/batch 314.29 | Loss 0.2786 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    42 /    47 | ms/batch 387.77 | Loss 0.2792 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    43 /    47 | ms/batch 401.77 | Loss 0.2577 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    44 /    47 | ms/batch 387.35 | Loss 0.2525 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    45 /    47 | ms/batch 359.01 | Loss 0.2518 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    46 /    47 | ms/batch 419.54 | Loss 0.2394 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:90: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:99: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Validation]| Epochs  99 | Elapsed 8813.33 | Loss 0.2401 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs  99 | Hit ratio 0.3871 | NDCG 0.3465 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     1 /    47 | ms/batch 776.22 | Loss 0.4611 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     2 /    47 | ms/batch 365.19 | Loss 0.2315 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     3 /    47 | ms/batch 329.46 | Loss 0.2249 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     4 /    47 | ms/batch 390.56 | Loss 0.2228 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     5 /    47 | ms/batch 376.94 | Loss 0.2241 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     6 /    47 | ms/batch 383.43 | Loss 0.2120 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     7 /    47 | ms/batch 370.94 | Loss 0.2100 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     8 /    47 | ms/batch 396.88 | Loss 0.2101 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     9 /    47 | ms/batch 379.47 | Loss 0.2149 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    10 /    47 | ms/batch 407.63 | Loss 0.1961 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    11 /    47 | ms/batch 379.16 | Loss 0.2082 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    12 /    47 | ms/batch 388.00 | Loss 0.1971 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    13 /    47 | ms/batch 387.67 | Loss 0.2051 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    14 /    47 | ms/batch 315.52 | Loss 0.1908 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    15 /    47 | ms/batch 377.31 | Loss 0.1863 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    16 /    47 | ms/batch 393.56 | Loss 0.1988 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    17 /    47 | ms/batch 378.43 | Loss 0.2089 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    18 /    47 | ms/batch 393.28 | Loss 0.1900 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    19 /    47 | ms/batch 363.49 | Loss 0.2065 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    20 /    47 | ms/batch 409.58 | Loss 0.1961 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 102 | Batch    21 /    47 | ms/batch 384.54 | Loss 0.1972 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    22 /    47 | ms/batch 337.93 | Loss 0.1914 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    23 /    47 | ms/batch 336.92 | Loss 0.1899 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    24 /    47 | ms/batch 358.81 | Loss 0.1906 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    25 /    47 | ms/batch 399.93 | Loss 0.1954 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    26 /    47 | ms/batch 312.33 | Loss 0.1936 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    27 /    47 | ms/batch 354.10 | Loss 0.1988 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    28 /    47 | ms/batch 338.81 | Loss 0.1904 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    29 /    47 | ms/batch 426.63 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    30 /    47 | ms/batch 344.27 | Loss 0.1892 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    31 /    47 | ms/batch 388.70 | Loss 0.1869 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    32 /    47 | ms/batch 390.02 | Loss 0.1921 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    33 /    47 | ms/batch 381.52 | Loss 0.1920 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    34 /    47 | ms/batch 401.26 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    35 /    47 | ms/batch 391.57 | Loss 0.1848 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    36 /    47 | ms/batch 341.89 | Loss 0.1786 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    37 /    47 | ms/batch 359.66 | Loss 0.1731 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    38 /    47 | ms/batch 393.49 | Loss 0.1856 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    39 /    47 | ms/batch 355.56 | Loss 0.1964 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    40 /    47 | ms/batch 418.72 | Loss 0.1770 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    41 /    47 | ms/batch 387.30 | Loss 0.1945 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    42 /    47 | ms/batch 355.97 | Loss 0.1832 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    43 /    47 | ms/batch 400.12 | Loss 0.1972 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    44 /    47 | ms/batch 403.73 | Loss 0.1808 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    45 /    47 | ms/batch 375.61 | Loss 0.1832 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    46 /    47 | ms/batch 352.70 | Loss 0.1865 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 102 | Elapsed 8837.50 | Loss 0.1841 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 102 | Hit ratio 0.4470 | NDCG 0.4061 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     1 /    47 | ms/batch 727.19 | Loss 0.3332 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     2 /    47 | ms/batch 343.67 | Loss 0.1693 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     3 /    47 | ms/batch 402.78 | Loss 0.1694 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     4 /    47 | ms/batch 377.87 | Loss 0.1798 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     5 /    47 | ms/batch 337.41 | Loss 0.1631 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     6 /    47 | ms/batch 355.37 | Loss 0.1666 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     7 /    47 | ms/batch 404.02 | Loss 0.1686 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     8 /    47 | ms/batch 406.10 | Loss 0.1789 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     9 /    47 | ms/batch 393.78 | Loss 0.1811 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    10 /    47 | ms/batch 410.62 | Loss 0.1757 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    11 /    47 | ms/batch 338.53 | Loss 0.1702 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    12 /    47 | ms/batch 408.09 | Loss 0.1762 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    13 /    47 | ms/batch 408.14 | Loss 0.1696 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    14 /    47 | ms/batch 335.13 | Loss 0.1816 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    15 /    47 | ms/batch 317.75 | Loss 0.1692 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    16 /    47 | ms/batch 387.64 | Loss 0.1729 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    17 /    47 | ms/batch 431.07 | Loss 0.1649 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    18 /    47 | ms/batch 382.33 | Loss 0.1747 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    19 /    47 | ms/batch 361.90 | Loss 0.1689 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    20 /    47 | ms/batch 408.67 | Loss 0.1735 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    21 /    47 | ms/batch 356.43 | Loss 0.1678 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    22 /    47 | ms/batch 385.28 | Loss 0.1720 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    23 /    47 | ms/batch 403.29 | Loss 0.1699 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    24 /    47 | ms/batch 385.73 | Loss 0.1705 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    25 /    47 | ms/batch 334.17 | Loss 0.1766 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    26 /    47 | ms/batch 324.79 | Loss 0.1712 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    27 /    47 | ms/batch 374.47 | Loss 0.1715 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    28 /    47 | ms/batch 364.25 | Loss 0.1624 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    29 /    47 | ms/batch 388.94 | Loss 0.1772 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    30 /    47 | ms/batch 390.96 | Loss 0.1660 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    31 /    47 | ms/batch 364.85 | Loss 0.1768 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    32 /    47 | ms/batch 384.61 | Loss 0.1787 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    33 /    47 | ms/batch 408.47 | Loss 0.1716 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    34 /    47 | ms/batch 298.07 | Loss 0.1590 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    35 /    47 | ms/batch 387.53 | Loss 0.1682 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    36 /    47 | ms/batch 397.61 | Loss 0.1848 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    37 /    47 | ms/batch 394.87 | Loss 0.1708 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    38 /    47 | ms/batch 380.81 | Loss 0.1703 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    39 /    47 | ms/batch 399.29 | Loss 0.1814 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    40 /    47 | ms/batch 341.42 | Loss 0.1670 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    41 /    47 | ms/batch 352.12 | Loss 0.1739 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    42 /    47 | ms/batch 384.61 | Loss 0.1625 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    43 /    47 | ms/batch 390.43 | Loss 0.1605 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    44 /    47 | ms/batch 391.20 | Loss 0.1713 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    45 /    47 | ms/batch 362.45 | Loss 0.1794 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    46 /    47 | ms/batch 406.96 | Loss 0.1637 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 104 | Elapsed 8773.33 | Loss 0.1790 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 104 | Hit ratio 0.4508 | NDCG 0.4130 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     1 /    47 | ms/batch 750.70 | Loss 0.3315 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     2 /    47 | ms/batch 397.60 | Loss 0.1614 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     3 /    47 | ms/batch 375.58 | Loss 0.1663 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     4 /    47 | ms/batch 396.39 | Loss 0.1679 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     5 /    47 | ms/batch 380.51 | Loss 0.1668 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     6 /    47 | ms/batch 382.12 | Loss 0.1767 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 106 | Batch     7 /    47 | ms/batch 336.46 | Loss 0.1614 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     8 /    47 | ms/batch 391.39 | Loss 0.1631 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     9 /    47 | ms/batch 332.78 | Loss 0.1614 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    10 /    47 | ms/batch 420.06 | Loss 0.1661 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    11 /    47 | ms/batch 397.97 | Loss 0.1649 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    12 /    47 | ms/batch 402.46 | Loss 0.1655 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    13 /    47 | ms/batch 359.98 | Loss 0.1660 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    14 /    47 | ms/batch 387.93 | Loss 0.1707 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    15 /    47 | ms/batch 347.17 | Loss 0.1651 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    16 /    47 | ms/batch 384.16 | Loss 0.1614 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    17 /    47 | ms/batch 423.43 | Loss 0.1700 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    18 /    47 | ms/batch 383.90 | Loss 0.1576 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    19 /    47 | ms/batch 409.71 | Loss 0.1673 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    20 /    47 | ms/batch 396.85 | Loss 0.1575 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    21 /    47 | ms/batch 369.13 | Loss 0.1581 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    22 /    47 | ms/batch 387.52 | Loss 0.1686 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    23 /    47 | ms/batch 394.93 | Loss 0.1569 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    24 /    47 | ms/batch 400.40 | Loss 0.1683 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    25 /    47 | ms/batch 342.27 | Loss 0.1664 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    26 /    47 | ms/batch 361.52 | Loss 0.1680 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    27 /    47 | ms/batch 388.20 | Loss 0.1804 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    28 /    47 | ms/batch 328.35 | Loss 0.1660 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    29 /    47 | ms/batch 364.01 | Loss 0.1681 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    30 /    47 | ms/batch 367.85 | Loss 0.1738 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    31 /    47 | ms/batch 397.86 | Loss 0.1685 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    32 /    47 | ms/batch 397.48 | Loss 0.1662 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    33 /    47 | ms/batch 357.91 | Loss 0.1732 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    34 /    47 | ms/batch 366.58 | Loss 0.1650 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    35 /    47 | ms/batch 387.16 | Loss 0.1703 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    36 /    47 | ms/batch 397.41 | Loss 0.1663 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    37 /    47 | ms/batch 361.88 | Loss 0.1696 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    38 /    47 | ms/batch 393.57 | Loss 0.1602 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    39 /    47 | ms/batch 421.51 | Loss 0.1673 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    40 /    47 | ms/batch 365.36 | Loss 0.1651 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    41 /    47 | ms/batch 387.27 | Loss 0.1609 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    42 /    47 | ms/batch 404.23 | Loss 0.1717 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    43 /    47 | ms/batch 396.81 | Loss 0.1651 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    44 /    47 | ms/batch 359.84 | Loss 0.1696 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    45 /    47 | ms/batch 353.97 | Loss 0.1693 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    46 /    47 | ms/batch 373.37 | Loss 0.1661 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 106 | Elapsed 8799.17 | Loss 0.1762 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 106 | Hit ratio 0.4509 | NDCG 0.4130 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Validation data processing...\")\n",
    "    validation_data = dh.load_data(Config().VALIDATIONSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Model config\n",
    "    model = DRModel(Config())\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config().learning_rate)\n",
    "\n",
    "    def bpr_loss(uids, baskets, dynamic_user, item_embedding):\n",
    "        \"\"\"\n",
    "        Bayesian personalized ranking loss for implicit feedback.\n",
    "\n",
    "        Args:\n",
    "            uids: batch of users' ID\n",
    "            baskets: batch of users' baskets\n",
    "            dynamic_user: batch of users' dynamic representations\n",
    "            item_embedding: item_embedding matrix\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for uid, bks, du in zip(uids, baskets, dynamic_user):\n",
    "            du_p_product = torch.mm(du, item_embedding.t())  # shape: [pad_len, num_item]\n",
    "            loss_u = []  # loss for user\n",
    "            for t, basket_t in enumerate(bks):\n",
    "                if basket_t[0] != 0 and t != 0:\n",
    "                    pos_idx = torch.cuda.LongTensor(basket_t) \n",
    "\n",
    "                    # Sample negative products\n",
    "                    neg = random.sample(list(neg_samples[uid]), len(basket_t))\n",
    "                    neg_idx = torch.cuda.LongTensor(neg) \n",
    "\n",
    "                    # Score p(u, t, v > v')\n",
    "                    score = du_p_product[t - 1][pos_idx] - du_p_product[t - 1][neg_idx]\n",
    "\n",
    "                    # Average Negative log likelihood for basket_t\n",
    "                    loss_u.append(torch.mean(-torch.nn.LogSigmoid()(score)))\n",
    "            for i in loss_u:\n",
    "                loss = loss + i / len(loss_u)\n",
    "        avg_loss = torch.div(loss, len(baskets))\n",
    "        return avg_loss\n",
    "\n",
    "    def train_model():\n",
    "        model.train()  # turn on training mode for dropout\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        train_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(train_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=True)):\n",
    "            uids, baskets, lens = x\n",
    "            model.zero_grad()  \n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid gradient exploding\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config().clip)\n",
    "\n",
    "            # Parameter updating\n",
    "            optimizer.step()\n",
    "            train_loss += loss.data\n",
    "\n",
    "            # Logging\n",
    "            if i % Config().log_interval == 0 and i > 0:\n",
    "                elapsed = (time.clock() - start_time) / Config().log_interval\n",
    "                cur_loss = train_loss.item() / Config().log_interval  # turn tensor into float\n",
    "                train_loss = 0\n",
    "                start_time = time.clock()\n",
    "                logger.info('[Training]| Epochs {:3d} | Batch {:5d} / {:5d} | ms/batch {:02.2f} | Loss {:05.4f} |'\n",
    "                            .format(epoch, i, num_batches, elapsed, cur_loss))\n",
    "\n",
    "    def validate_model():\n",
    "        model.eval()\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        val_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(validation_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(validation_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            val_loss += loss.data\n",
    "\n",
    "        # Logging\n",
    "        elapsed = (time.clock() - start_time) * 1000 / num_batches\n",
    "        val_loss = val_loss.item() / num_batches\n",
    "        logger.info('[Validation]| Epochs {:3d} | Elapsed {:02.2f} | Loss {:05.4f} |'\n",
    "                    .format(epoch, elapsed, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def test_model():\n",
    "        model.eval()\n",
    "        item_embedding = model.encode.weight\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "\n",
    "        hitratio_numer = 0\n",
    "        hitratio_denom = 0\n",
    "        ndcg = 0.0\n",
    "\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "                scores = []\n",
    "                du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "                # calculating <u,p> score for all test items <u,p> pair\n",
    "                positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "                p_length = len(positives)\n",
    "                positives = torch.cuda.LongTensor(positives)\n",
    "\n",
    "                # Deal with positives samples\n",
    "                scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "                for s in scores_pos:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Deal with negative samples\n",
    "                negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "                negtives = torch.cuda.LongTensor(negtives) \n",
    "                scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "                for s in scores_neg:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Calculate hit-ratio\n",
    "                index_k = []\n",
    "                for k in range(Config().top_k):\n",
    "                    index = scores.index(max(scores))\n",
    "                    index_k.append(index)\n",
    "                    scores[index] = -9999\n",
    "                hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "                hitratio_denom += p_length\n",
    "\n",
    "                # Calculate NDCG\n",
    "                u_dcg = 0\n",
    "                u_idcg = 0\n",
    "                for k in range(Config().top_k):\n",
    "                    if index_k[k] < p_length:  \n",
    "                        u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                    u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                ndcg += u_dcg / u_idcg\n",
    "\n",
    "        hit_ratio = hitratio_numer / hitratio_denom\n",
    "        ndcg = ndcg / len(train_data)\n",
    "        logger.info('[Test]| Epochs {:3d} | Hit ratio {:02.4f} | NDCG {:05.4f} |'\n",
    "                    .format(epoch, hit_ratio, ndcg))\n",
    "        return hit_ratio, ndcg\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    logger.info('Save into {0}'.format(out_dir))\n",
    "    checkpoint_dir = out_dir + '/model-{epoch:02d}-{hitratio:.4f}-{ndcg:.4f}.model'\n",
    "\n",
    "    best_hit_ratio = None\n",
    "\n",
    "    try:\n",
    "        # Training\n",
    "        for epoch in [99, 102, 104, 106]: #range(100, Config().epochs):\n",
    "            train_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            val_loss = validate_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            hit_ratio, ndcg = test_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            # Checkpoint\n",
    "            if not best_hit_ratio or hit_ratio > best_hit_ratio:\n",
    "                with open(checkpoint_dir.format(epoch=epoch, hitratio=hit_ratio, ndcg=ndcg), 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_hit_ratio = hit_ratio\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('*' * 89)\n",
    "        logger.info('Early Stopping!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☛ Please input the model file you want to test: 1605813697\n",
      "Hit ratio[15]: 0.4014114566000608\n",
      "NDCG[15]: 0.359027566598392\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from configc2 import Config\n",
    "import data_helper as dh\n",
    "\n",
    "\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/test-{0}.log\".format(time.asctime()))\n",
    "\n",
    "MODEL = input(\"☛ Please input the model file you want to test: \")\n",
    "\n",
    "while not (MODEL.isdigit() and len(MODEL) == 10):\n",
    "    MODEL = input(\"✘ The format of your input is illegal, it should be like(1490175368), please re-input: \")\n",
    "logger.info(\"✔︎ The format of your input is legal, now loading to next step...\")\n",
    "\n",
    "MODEL_DIR = dh.load_model_file(MODEL)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Load model\n",
    "    dr_model = torch.load(MODEL_DIR)\n",
    "\n",
    "    dr_model.eval()\n",
    "\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    hidden = dr_model.init_hidden(Config().batch_size)\n",
    "\n",
    "    hitratio_numer = 0\n",
    "    hitratio_denom = 0\n",
    "    ndcg = 0.0\n",
    "    results = []\n",
    "\n",
    "    for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "        uids, baskets, lens = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, hidden)\n",
    "        for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "            scores = []\n",
    "            du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "            # calculating <u,p> score for all test items <u,p> pair\n",
    "            positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "            p_length = len(positives)\n",
    "            positives = torch.LongTensor(positives)\n",
    "\n",
    "            # Deal with positives samples\n",
    "            scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "            for s in scores_pos:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Deal with negative samples\n",
    "            negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "            negtives = torch.LongTensor(negtives)\n",
    "            scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "            for s in scores_neg:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Calculate hit-ratio\n",
    "            index_k = []\n",
    "            for k in range(Config().top_k):\n",
    "                index = scores.index(max(scores))\n",
    "                index_k.append(index)\n",
    "                scores[index] = -9999\n",
    "            single_hit = len((set(np.arange(0, p_length)) & set(index_k)))/p_length\n",
    "            results.append([uid,index_k, set(np.arange(0, p_length)), single_hit])\n",
    "            hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "            hitratio_denom += p_length\n",
    "\n",
    "            # Calculate NDCG\n",
    "            u_dcg = 0\n",
    "            u_idcg = 0\n",
    "            for k in range(Config().top_k):\n",
    "                if index_k[k] < p_length:  \n",
    "                    u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "            ndcg += u_dcg / u_idcg\n",
    "\n",
    "    hitratio = hitratio_numer / hitratio_denom\n",
    "    ndcg = ndcg / len(train_data)\n",
    "    print('Hit ratio[{0}]: {1}'.format(Config().top_k, hitratio))\n",
    "    print('NDCG[{0}]: {1}'.format(Config().top_k, ndcg))\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results_ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results_, columns=['UserID', 'Prediction', 'Actual', 'Hit-Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confi_cut(score):\n",
    "    if score >= 0.0 and score <0.2:\n",
    "        return '0-0.2'\n",
    "    elif score >= 0.2 and score <0.4:\n",
    "        return '0.2-0.4'\n",
    "    elif score >= 0.4 and score <0.6:\n",
    "        return '0.4-0.6'\n",
    "    elif score >= 0.6 and score <0.8:\n",
    "        return '0.6-0.8'\n",
    "    else:\n",
    "        return '0.8-1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['confi'] = result_df['Hit-Ratio'].apply(lambda x: confi_cut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results/final_cluster_1_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
