{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to create a recommendation engine (DREAM) for cluster 0 from customer segmentation of instacart dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "   * Input: sequence of baskets of cluster 0 customers\n",
    "   * Negative sample : The food items which is never purchased by customers\n",
    "   \n",
    "### Evaluation Metric (Top 15 items)\n",
    "   * Hit Rate @15\n",
    "       - Counts the fraction of times that the ground truth next item is among the top 15 items.\n",
    "       - we only have one test item for each user, Hit@15 is equivalent to Recall@15\n",
    "       - It is also propotional to Precision@15\n",
    "   * NDCG@15\n",
    "       - A position aware metric with assigns larger weights on higher positions.\n",
    "       \n",
    "### Model \n",
    "   * Model is saved under runs folder with this key 1605947995\n",
    "   * This key 1605947995 is required to run it on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import data_helper as dh\n",
    "from configc1 import Config\n",
    "from rnn_model import DRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"✔︎ DREAM Model Training...\")\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/training-{0}.log\".format(time.asctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:torch-log:                                         MODEL_DIR|runs/                                             \n",
      "INFO:torch-log:                                       NEG_SAMPLES|../data/neg_sample_insta_all_sampled_b4.pickle    \n",
      "INFO:torch-log:                                       TESTSET_DIR|../data/allb4_cluster0_test.json                  \n",
      "INFO:torch-log:                                   TRAININGSET_DIR|../data/allb4_cluster0_train.json                 \n",
      "INFO:torch-log:                                 VALIDATIONSET_DIR|../data/allb4_cluster0_val.json                   \n",
      "INFO:torch-log:                                  BASKET_POOL_TYPE|max                                               \n",
      "INFO:torch-log:                                        BATCH_SIZE|1000                                              \n",
      "INFO:torch-log:                                              CLIP|10                                                \n",
      "INFO:torch-log:                                              CUDA|1                                                 \n",
      "INFO:torch-log:                                           DROPOUT|0.5                                               \n",
      "INFO:torch-log:                                     EMBEDDING_DIM|32                                                \n",
      "INFO:torch-log:                                            EPOCHS|111                                               \n",
      "INFO:torch-log:                                     LEARNING_RATE|0.01                                              \n",
      "INFO:torch-log:                                      LOG_INTERVAL|1                                                 \n",
      "INFO:torch-log:                                           NEG_NUM|500                                               \n",
      "INFO:torch-log:                                       NUM_PRODUCT|58925                                             \n",
      "INFO:torch-log:                                     RNN_LAYER_NUM|2                                                 \n",
      "INFO:torch-log:                                          RNN_TYPE|LSTM                                              \n",
      "INFO:torch-log:                                           SEQ_LEN|99                                                \n",
      "INFO:torch-log:                                             TOP_K|15                                                \n",
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dilim = '-' * 120\n",
    "logger.info(dilim)\n",
    "for attr in sorted(Config().__dict__):\n",
    "    logger.info('{:>50}|{:<50}'.format(attr.upper(), Config().__dict__[attr]))\n",
    "logger.info(dilim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Validation data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n",
      "INFO:torch-log:Save into /home/reshmask/Next-Basket-Recommendation-master/DREAM/runs/1605947995\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:82: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     1 /    72 | ms/batch 1096.92 | Loss 1.4831 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     2 /    72 | ms/batch 640.23 | Loss 0.6972 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     3 /    72 | ms/batch 639.11 | Loss 0.6852 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     4 /    72 | ms/batch 665.11 | Loss 0.6822 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     5 /    72 | ms/batch 649.14 | Loss 0.6758 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     6 /    72 | ms/batch 647.37 | Loss 0.6717 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     7 /    72 | ms/batch 673.11 | Loss 0.6676 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     8 /    72 | ms/batch 601.16 | Loss 0.6589 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     9 /    72 | ms/batch 650.95 | Loss 0.6549 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    10 /    72 | ms/batch 651.33 | Loss 0.6476 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    11 /    72 | ms/batch 649.19 | Loss 0.6397 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    12 /    72 | ms/batch 665.68 | Loss 0.6281 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    13 /    72 | ms/batch 654.85 | Loss 0.6186 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    14 /    72 | ms/batch 613.18 | Loss 0.6087 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    15 /    72 | ms/batch 605.82 | Loss 0.5988 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    16 /    72 | ms/batch 650.16 | Loss 0.5833 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    17 /    72 | ms/batch 610.36 | Loss 0.5685 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    18 /    72 | ms/batch 594.11 | Loss 0.5623 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    19 /    72 | ms/batch 609.87 | Loss 0.5420 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    20 /    72 | ms/batch 612.21 | Loss 0.5283 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    21 /    72 | ms/batch 633.85 | Loss 0.5137 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    22 /    72 | ms/batch 644.06 | Loss 0.5039 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    23 /    72 | ms/batch 638.05 | Loss 0.4911 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    24 /    72 | ms/batch 626.71 | Loss 0.4686 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    25 /    72 | ms/batch 602.09 | Loss 0.4567 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    26 /    72 | ms/batch 610.17 | Loss 0.4478 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    27 /    72 | ms/batch 633.67 | Loss 0.4332 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    28 /    72 | ms/batch 624.82 | Loss 0.4196 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    29 /    72 | ms/batch 583.68 | Loss 0.3958 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    30 /    72 | ms/batch 579.59 | Loss 0.3901 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    31 /    72 | ms/batch 596.22 | Loss 0.3749 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    32 /    72 | ms/batch 625.82 | Loss 0.3661 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    33 /    72 | ms/batch 619.40 | Loss 0.3528 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    34 /    72 | ms/batch 598.20 | Loss 0.3473 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    35 /    72 | ms/batch 608.08 | Loss 0.3303 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    36 /    72 | ms/batch 601.82 | Loss 0.3282 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    37 /    72 | ms/batch 628.88 | Loss 0.3206 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    38 /    72 | ms/batch 627.27 | Loss 0.2930 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    39 /    72 | ms/batch 610.28 | Loss 0.3018 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    40 /    72 | ms/batch 605.94 | Loss 0.2903 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    41 /    72 | ms/batch 633.19 | Loss 0.2860 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    42 /    72 | ms/batch 618.06 | Loss 0.2742 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    43 /    72 | ms/batch 614.28 | Loss 0.2647 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    44 /    72 | ms/batch 607.56 | Loss 0.2600 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    45 /    72 | ms/batch 624.60 | Loss 0.2517 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    46 /    72 | ms/batch 620.04 | Loss 0.2584 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    47 /    72 | ms/batch 616.13 | Loss 0.2431 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    48 /    72 | ms/batch 595.60 | Loss 0.2470 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    49 /    72 | ms/batch 613.25 | Loss 0.2412 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    50 /    72 | ms/batch 642.82 | Loss 0.2372 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    51 /    72 | ms/batch 631.31 | Loss 0.2411 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    52 /    72 | ms/batch 608.13 | Loss 0.2327 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    53 /    72 | ms/batch 615.53 | Loss 0.2343 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    54 /    72 | ms/batch 604.68 | Loss 0.2243 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    55 /    72 | ms/batch 557.63 | Loss 0.2259 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    56 /    72 | ms/batch 618.56 | Loss 0.2285 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    57 /    72 | ms/batch 572.25 | Loss 0.2219 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    58 /    72 | ms/batch 612.89 | Loss 0.2171 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    59 /    72 | ms/batch 540.15 | Loss 0.2155 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    60 /    72 | ms/batch 597.40 | Loss 0.2172 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    61 /    72 | ms/batch 530.38 | Loss 0.2162 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    62 /    72 | ms/batch 612.30 | Loss 0.2078 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    63 /    72 | ms/batch 610.18 | Loss 0.2187 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    64 /    72 | ms/batch 607.16 | Loss 0.2078 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    65 /    72 | ms/batch 620.82 | Loss 0.2062 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    66 /    72 | ms/batch 611.82 | Loss 0.2087 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    67 /    72 | ms/batch 549.20 | Loss 0.2113 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    68 /    72 | ms/batch 470.86 | Loss 0.2077 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    69 /    72 | ms/batch 642.18 | Loss 0.2045 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    70 /    72 | ms/batch 629.22 | Loss 0.2040 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    71 /    72 | ms/batch 605.97 | Loss 0.1957 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:90: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:99: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Validation]| Epochs  99 | Elapsed 16215.00 | Loss 0.2005 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs  99 | Hit ratio 0.4450 | NDCG 0.3406 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     1 /    72 | ms/batch 1136.24 | Loss 0.3954 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     2 /    72 | ms/batch 592.24 | Loss 0.2010 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     3 /    72 | ms/batch 577.14 | Loss 0.1946 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     4 /    72 | ms/batch 589.33 | Loss 0.2017 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     5 /    72 | ms/batch 601.45 | Loss 0.2019 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     6 /    72 | ms/batch 428.66 | Loss 0.2015 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     7 /    72 | ms/batch 574.06 | Loss 0.2007 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     8 /    72 | ms/batch 538.73 | Loss 0.1921 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     9 /    72 | ms/batch 588.57 | Loss 0.1972 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    10 /    72 | ms/batch 439.44 | Loss 0.1907 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    11 /    72 | ms/batch 588.64 | Loss 0.1937 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    12 /    72 | ms/batch 577.03 | Loss 0.1960 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    13 /    72 | ms/batch 580.13 | Loss 0.1905 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    14 /    72 | ms/batch 570.92 | Loss 0.1860 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    15 /    72 | ms/batch 578.18 | Loss 0.1983 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    16 /    72 | ms/batch 602.77 | Loss 0.1790 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    17 /    72 | ms/batch 567.22 | Loss 0.1845 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    18 /    72 | ms/batch 600.07 | Loss 0.2005 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    19 /    72 | ms/batch 518.24 | Loss 0.1916 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    20 /    72 | ms/batch 593.95 | Loss 0.1889 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    21 /    72 | ms/batch 441.84 | Loss 0.1920 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    22 /    72 | ms/batch 582.48 | Loss 0.1955 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    23 /    72 | ms/batch 617.26 | Loss 0.1860 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    24 /    72 | ms/batch 589.28 | Loss 0.1929 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    25 /    72 | ms/batch 582.10 | Loss 0.1926 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    26 /    72 | ms/batch 608.92 | Loss 0.1876 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    27 /    72 | ms/batch 419.42 | Loss 0.1884 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    28 /    72 | ms/batch 533.73 | Loss 0.1853 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    29 /    72 | ms/batch 563.63 | Loss 0.1915 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    30 /    72 | ms/batch 582.11 | Loss 0.1868 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    31 /    72 | ms/batch 597.30 | Loss 0.1839 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    32 /    72 | ms/batch 566.31 | Loss 0.1900 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    33 /    72 | ms/batch 591.08 | Loss 0.1877 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    34 /    72 | ms/batch 592.19 | Loss 0.1869 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    35 /    72 | ms/batch 601.37 | Loss 0.1880 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    36 /    72 | ms/batch 446.30 | Loss 0.1878 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    37 /    72 | ms/batch 613.48 | Loss 0.1913 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    38 /    72 | ms/batch 616.70 | Loss 0.1861 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    39 /    72 | ms/batch 605.72 | Loss 0.1925 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    40 /    72 | ms/batch 583.25 | Loss 0.1902 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    41 /    72 | ms/batch 580.77 | Loss 0.1879 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    42 /    72 | ms/batch 590.53 | Loss 0.1878 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    43 /    72 | ms/batch 615.03 | Loss 0.1819 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    44 /    72 | ms/batch 591.00 | Loss 0.1856 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    45 /    72 | ms/batch 580.53 | Loss 0.1873 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    46 /    72 | ms/batch 458.79 | Loss 0.1858 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    47 /    72 | ms/batch 581.20 | Loss 0.1887 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    48 /    72 | ms/batch 599.53 | Loss 0.1962 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    49 /    72 | ms/batch 596.99 | Loss 0.1889 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    50 /    72 | ms/batch 587.91 | Loss 0.1981 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    51 /    72 | ms/batch 614.26 | Loss 0.1882 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    52 /    72 | ms/batch 582.09 | Loss 0.1889 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    53 /    72 | ms/batch 590.59 | Loss 0.1877 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    54 /    72 | ms/batch 615.51 | Loss 0.1833 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    55 /    72 | ms/batch 574.85 | Loss 0.1835 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    56 /    72 | ms/batch 573.41 | Loss 0.1944 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    57 /    72 | ms/batch 579.83 | Loss 0.1964 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    58 /    72 | ms/batch 582.44 | Loss 0.1864 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    59 /    72 | ms/batch 581.86 | Loss 0.1903 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    60 /    72 | ms/batch 582.49 | Loss 0.1837 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    61 /    72 | ms/batch 614.70 | Loss 0.1854 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    62 /    72 | ms/batch 575.66 | Loss 0.1940 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    63 /    72 | ms/batch 589.06 | Loss 0.1873 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    64 /    72 | ms/batch 426.02 | Loss 0.1933 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    65 /    72 | ms/batch 439.02 | Loss 0.1879 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    66 /    72 | ms/batch 606.67 | Loss 0.1771 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    67 /    72 | ms/batch 589.82 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    68 /    72 | ms/batch 593.04 | Loss 0.1865 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    69 /    72 | ms/batch 565.29 | Loss 0.1867 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    70 /    72 | ms/batch 604.49 | Loss 0.1855 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    71 /    72 | ms/batch 435.38 | Loss 0.1859 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 102 | Elapsed 16404.44 | Loss 0.1862 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 102 | Hit ratio 0.4613 | NDCG 0.3592 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     1 /    72 | ms/batch 1194.24 | Loss 0.3634 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     2 /    72 | ms/batch 605.42 | Loss 0.1853 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     3 /    72 | ms/batch 428.25 | Loss 0.1883 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     4 /    72 | ms/batch 432.38 | Loss 0.1812 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     5 /    72 | ms/batch 589.46 | Loss 0.1723 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     6 /    72 | ms/batch 590.70 | Loss 0.1831 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     7 /    72 | ms/batch 457.76 | Loss 0.1850 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     8 /    72 | ms/batch 585.15 | Loss 0.1812 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     9 /    72 | ms/batch 580.93 | Loss 0.1848 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 104 | Batch    10 /    72 | ms/batch 589.22 | Loss 0.1862 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    11 /    72 | ms/batch 600.08 | Loss 0.1813 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    12 /    72 | ms/batch 570.74 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    13 /    72 | ms/batch 596.17 | Loss 0.1848 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    14 /    72 | ms/batch 593.38 | Loss 0.1874 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    15 /    72 | ms/batch 603.06 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    16 /    72 | ms/batch 587.35 | Loss 0.1776 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    17 /    72 | ms/batch 586.27 | Loss 0.1846 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    18 /    72 | ms/batch 596.66 | Loss 0.1863 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    19 /    72 | ms/batch 615.72 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    20 /    72 | ms/batch 536.08 | Loss 0.1815 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    21 /    72 | ms/batch 566.90 | Loss 0.1775 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    22 /    72 | ms/batch 598.46 | Loss 0.1827 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    23 /    72 | ms/batch 422.29 | Loss 0.1822 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    24 /    72 | ms/batch 540.52 | Loss 0.1812 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    25 /    72 | ms/batch 595.91 | Loss 0.1811 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    26 /    72 | ms/batch 596.85 | Loss 0.1813 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    27 /    72 | ms/batch 615.18 | Loss 0.1799 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    28 /    72 | ms/batch 577.55 | Loss 0.1847 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    29 /    72 | ms/batch 612.14 | Loss 0.1787 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    30 /    72 | ms/batch 432.92 | Loss 0.1800 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    31 /    72 | ms/batch 591.29 | Loss 0.1845 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    32 /    72 | ms/batch 586.33 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    33 /    72 | ms/batch 602.15 | Loss 0.1790 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    34 /    72 | ms/batch 557.06 | Loss 0.1805 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    35 /    72 | ms/batch 580.51 | Loss 0.1813 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    36 /    72 | ms/batch 452.97 | Loss 0.1839 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    37 /    72 | ms/batch 439.36 | Loss 0.1845 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    38 /    72 | ms/batch 592.74 | Loss 0.1877 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    39 /    72 | ms/batch 592.69 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    40 /    72 | ms/batch 422.81 | Loss 0.1813 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    41 /    72 | ms/batch 601.72 | Loss 0.1807 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    42 /    72 | ms/batch 518.06 | Loss 0.1838 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    43 /    72 | ms/batch 592.93 | Loss 0.1847 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    44 /    72 | ms/batch 585.89 | Loss 0.1752 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    45 /    72 | ms/batch 629.21 | Loss 0.1871 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    46 /    72 | ms/batch 589.25 | Loss 0.1743 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    47 /    72 | ms/batch 597.75 | Loss 0.1894 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    48 /    72 | ms/batch 573.66 | Loss 0.1793 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    49 /    72 | ms/batch 597.82 | Loss 0.1835 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    50 /    72 | ms/batch 594.85 | Loss 0.1840 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    51 /    72 | ms/batch 586.50 | Loss 0.1889 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    52 /    72 | ms/batch 600.53 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    53 /    72 | ms/batch 579.41 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    54 /    72 | ms/batch 433.91 | Loss 0.1907 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    55 /    72 | ms/batch 581.91 | Loss 0.1922 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    56 /    72 | ms/batch 597.49 | Loss 0.1782 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    57 /    72 | ms/batch 564.25 | Loss 0.1830 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    58 /    72 | ms/batch 600.27 | Loss 0.1788 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    59 /    72 | ms/batch 584.57 | Loss 0.1834 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    60 /    72 | ms/batch 602.49 | Loss 0.1796 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    61 /    72 | ms/batch 696.45 | Loss 0.1828 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    62 /    72 | ms/batch 582.41 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    63 /    72 | ms/batch 569.36 | Loss 0.1773 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    64 /    72 | ms/batch 577.01 | Loss 0.1886 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    65 /    72 | ms/batch 455.46 | Loss 0.1792 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    66 /    72 | ms/batch 533.73 | Loss 0.1785 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    67 /    72 | ms/batch 592.51 | Loss 0.1817 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    68 /    72 | ms/batch 596.33 | Loss 0.1850 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    69 /    72 | ms/batch 595.31 | Loss 0.1839 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    70 /    72 | ms/batch 593.66 | Loss 0.1835 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    71 /    72 | ms/batch 605.62 | Loss 0.1778 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 104 | Elapsed 16301.11 | Loss 0.1834 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 104 | Hit ratio 0.4623 | NDCG 0.3616 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     1 /    72 | ms/batch 1162.27 | Loss 0.3653 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     2 /    72 | ms/batch 534.61 | Loss 0.1732 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     3 /    72 | ms/batch 587.88 | Loss 0.1768 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     4 /    72 | ms/batch 607.65 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     5 /    72 | ms/batch 576.61 | Loss 0.1731 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     6 /    72 | ms/batch 593.20 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     7 /    72 | ms/batch 427.41 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     8 /    72 | ms/batch 566.93 | Loss 0.1747 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     9 /    72 | ms/batch 577.10 | Loss 0.1736 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    10 /    72 | ms/batch 449.42 | Loss 0.1822 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    11 /    72 | ms/batch 606.87 | Loss 0.1739 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    12 /    72 | ms/batch 599.53 | Loss 0.1720 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    13 /    72 | ms/batch 703.52 | Loss 0.1769 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    14 /    72 | ms/batch 642.92 | Loss 0.1846 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    15 /    72 | ms/batch 624.35 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    16 /    72 | ms/batch 450.98 | Loss 0.1872 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    17 /    72 | ms/batch 427.97 | Loss 0.1716 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    18 /    72 | ms/batch 447.95 | Loss 0.1723 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    19 /    72 | ms/batch 438.48 | Loss 0.1720 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    20 /    72 | ms/batch 441.20 | Loss 0.1798 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    21 /    72 | ms/batch 441.75 | Loss 0.1885 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 106 | Batch    22 /    72 | ms/batch 454.89 | Loss 0.1818 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    23 /    72 | ms/batch 428.83 | Loss 0.1769 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    24 /    72 | ms/batch 441.44 | Loss 0.1796 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    25 /    72 | ms/batch 438.93 | Loss 0.1828 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    26 /    72 | ms/batch 452.40 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    27 /    72 | ms/batch 476.77 | Loss 0.1783 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    28 /    72 | ms/batch 434.47 | Loss 0.1745 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    29 /    72 | ms/batch 449.83 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    30 /    72 | ms/batch 435.51 | Loss 0.1732 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    31 /    72 | ms/batch 445.90 | Loss 0.1750 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    32 /    72 | ms/batch 464.53 | Loss 0.1838 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    33 /    72 | ms/batch 438.66 | Loss 0.1737 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    34 /    72 | ms/batch 438.09 | Loss 0.1831 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    35 /    72 | ms/batch 455.20 | Loss 0.1758 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    36 /    72 | ms/batch 440.76 | Loss 0.1811 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    37 /    72 | ms/batch 444.08 | Loss 0.1821 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    38 /    72 | ms/batch 457.51 | Loss 0.1825 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    39 /    72 | ms/batch 442.96 | Loss 0.1746 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    40 /    72 | ms/batch 432.62 | Loss 0.1763 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    41 /    72 | ms/batch 448.63 | Loss 0.1777 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    42 /    72 | ms/batch 413.79 | Loss 0.1861 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    43 /    72 | ms/batch 444.63 | Loss 0.1833 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    44 /    72 | ms/batch 443.99 | Loss 0.1864 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    45 /    72 | ms/batch 441.79 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    46 /    72 | ms/batch 442.28 | Loss 0.1786 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    47 /    72 | ms/batch 445.28 | Loss 0.1781 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    48 /    72 | ms/batch 423.03 | Loss 0.1803 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    49 /    72 | ms/batch 444.93 | Loss 0.1763 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    50 /    72 | ms/batch 446.13 | Loss 0.1806 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    51 /    72 | ms/batch 450.28 | Loss 0.1700 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    52 /    72 | ms/batch 439.18 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    53 /    72 | ms/batch 439.17 | Loss 0.1741 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    54 /    72 | ms/batch 439.14 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    55 /    72 | ms/batch 425.36 | Loss 0.1837 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    56 /    72 | ms/batch 451.19 | Loss 0.1800 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    57 /    72 | ms/batch 448.81 | Loss 0.1748 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    58 /    72 | ms/batch 427.33 | Loss 0.1844 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    59 /    72 | ms/batch 447.65 | Loss 0.1822 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    60 /    72 | ms/batch 430.33 | Loss 0.1777 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    61 /    72 | ms/batch 436.36 | Loss 0.1714 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    62 /    72 | ms/batch 433.63 | Loss 0.1825 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    63 /    72 | ms/batch 461.44 | Loss 0.1806 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    64 /    72 | ms/batch 445.51 | Loss 0.1808 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    65 /    72 | ms/batch 432.36 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    66 /    72 | ms/batch 441.37 | Loss 0.1765 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    67 /    72 | ms/batch 439.41 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    68 /    72 | ms/batch 448.73 | Loss 0.1838 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    69 /    72 | ms/batch 443.67 | Loss 0.1825 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    70 /    72 | ms/batch 430.34 | Loss 0.1854 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    71 /    72 | ms/batch 448.89 | Loss 0.1668 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 106 | Elapsed 16217.78 | Loss 0.1818 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 106 | Hit ratio 0.4668 | NDCG 0.3645 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Validation data processing...\")\n",
    "    validation_data = dh.load_data(Config().VALIDATIONSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Model config\n",
    "    model = DRModel(Config())\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config().learning_rate)\n",
    "\n",
    "    def bpr_loss(uids, baskets, dynamic_user, item_embedding):\n",
    "        \"\"\"\n",
    "        Bayesian personalized ranking loss for implicit feedback.\n",
    "\n",
    "        Args:\n",
    "            uids: batch of users' ID\n",
    "            baskets: batch of users' baskets\n",
    "            dynamic_user: batch of users' dynamic representations\n",
    "            item_embedding: item_embedding matrix\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for uid, bks, du in zip(uids, baskets, dynamic_user):\n",
    "            du_p_product = torch.mm(du, item_embedding.t())  # shape: [pad_len, num_item]\n",
    "            loss_u = []  # loss for user\n",
    "            for t, basket_t in enumerate(bks):\n",
    "                if basket_t[0] != 0 and t != 0:\n",
    "                    pos_idx = torch.cuda.LongTensor(basket_t)\n",
    "\n",
    "                    # Sample negative products\n",
    "                    neg = random.sample(list(neg_samples[uid]), len(basket_t))\n",
    "                    neg_idx = torch.cuda.LongTensor(neg)\n",
    "                    # Score p(u, t, v > v')\n",
    "                    score = du_p_product[t - 1][pos_idx] - du_p_product[t - 1][neg_idx]\n",
    "\n",
    "                    # Average Negative log likelihood for basket_t\n",
    "                    loss_u.append(torch.mean(-torch.nn.LogSigmoid()(score)))\n",
    "            for i in loss_u:\n",
    "                loss = loss + i / len(loss_u)\n",
    "        avg_loss = torch.div(loss, len(baskets))\n",
    "        return avg_loss\n",
    "\n",
    "    def train_model():\n",
    "        model.train()  # turn on training mode for dropout\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        train_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(train_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=True)):\n",
    "            uids, baskets, lens = x\n",
    "            model.zero_grad() \n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid gradient exploding\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config().clip)\n",
    "\n",
    "            # Parameter updating\n",
    "            optimizer.step()\n",
    "            train_loss += loss.data\n",
    "\n",
    "            # Logging\n",
    "            if i % Config().log_interval == 0 and i > 0:\n",
    "                elapsed = (time.clock() - start_time) / Config().log_interval\n",
    "                cur_loss = train_loss.item() / Config().log_interval  # turn tensor into float\n",
    "                train_loss = 0\n",
    "                start_time = time.clock()\n",
    "                logger.info('[Training]| Epochs {:3d} | Batch {:5d} / {:5d} | ms/batch {:02.2f} | Loss {:05.4f} |'\n",
    "                            .format(epoch, i, num_batches, elapsed, cur_loss))\n",
    "\n",
    "    def validate_model():\n",
    "        model.eval()\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        val_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(validation_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(validation_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            val_loss += loss.data\n",
    "\n",
    "        # Logging\n",
    "        elapsed = (time.clock() - start_time) * 1000 / num_batches\n",
    "        val_loss = val_loss.item() / num_batches\n",
    "        logger.info('[Validation]| Epochs {:3d} | Elapsed {:02.2f} | Loss {:05.4f} |'\n",
    "                    .format(epoch, elapsed, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def test_model():\n",
    "        model.eval()\n",
    "        item_embedding = model.encode.weight\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "\n",
    "        hitratio_numer = 0\n",
    "        hitratio_denom = 0\n",
    "        ndcg = 0.0\n",
    "\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "                scores = []\n",
    "                du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "                # calculating <u,p> score for all test items <u,p> pair\n",
    "                positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "                p_length = len(positives)\n",
    "                positives = torch.cuda.LongTensor(positives) \n",
    "\n",
    "                # Deal with positives samples\n",
    "                scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "                for s in scores_pos:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Deal with negative samples\n",
    "                negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "                negtives = torch.cuda.LongTensor(negtives) \n",
    "                scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "                for s in scores_neg:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Calculate hit-ratio\n",
    "                index_k = []\n",
    "                for k in range(Config().top_k):\n",
    "                    index = scores.index(max(scores))\n",
    "                    index_k.append(index)\n",
    "                    scores[index] = -9999\n",
    "                hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "                hitratio_denom += p_length\n",
    "\n",
    "                # Calculate NDCG\n",
    "                u_dcg = 0\n",
    "                u_idcg = 0\n",
    "                for k in range(Config().top_k):\n",
    "                    if index_k[k] < p_length:  \n",
    "                        u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                    u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                ndcg += u_dcg / u_idcg\n",
    "\n",
    "        hit_ratio = hitratio_numer / hitratio_denom\n",
    "        ndcg = ndcg / len(train_data)\n",
    "        logger.info('[Test]| Epochs {:3d} | Hit ratio {:02.4f} | NDCG {:05.4f} |'\n",
    "                    .format(epoch, hit_ratio, ndcg))\n",
    "        return hit_ratio, ndcg\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    logger.info('Save into {0}'.format(out_dir))\n",
    "    checkpoint_dir = out_dir + '/model-{epoch:02d}-{hitratio:.4f}-{ndcg:.4f}.model'\n",
    "\n",
    "    best_hit_ratio = None\n",
    "\n",
    "    try:\n",
    "        # Training\n",
    "        for epoch in [99, 102, 104, 106]: #range(100, Config().epochs):\n",
    "            train_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            val_loss = validate_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            hit_ratio, ndcg = test_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            # Checkpoint\n",
    "            if not best_hit_ratio or hit_ratio > best_hit_ratio:\n",
    "                with open(checkpoint_dir.format(epoch=epoch, hitratio=hit_ratio, ndcg=ndcg), 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_hit_ratio = hit_ratio\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('*' * 89)\n",
    "        logger.info('Early Stopping!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☛ Please input the model file you want to test: 1605947995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ The format of your input is legal, now loading to next step...\n",
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio[15]: 0.44505490463235303\n",
      "NDCG[15]: 0.34058474166888647\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from configc1 import Config\n",
    "import data_helper as dh\n",
    "\n",
    "\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/test-{0}.log\".format(time.asctime()))\n",
    "\n",
    "MODEL = input(\"☛ Please input the model file you want to test: \")\n",
    "\n",
    "while not (MODEL.isdigit() and len(MODEL) == 10):\n",
    "    MODEL = input(\"✘ The format of your input is illegal, it should be like(1490175368), please re-input: \")\n",
    "logger.info(\"✔︎ The format of your input is legal, now loading to next step...\")\n",
    "\n",
    "MODEL_DIR = dh.load_model_file(MODEL)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Load model\n",
    "    dr_model = torch.load(MODEL_DIR)\n",
    "\n",
    "    dr_model.eval()\n",
    "\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    hidden = dr_model.init_hidden(Config().batch_size)\n",
    "\n",
    "    hitratio_numer = 0\n",
    "    hitratio_denom = 0\n",
    "    ndcg = 0.0\n",
    "    results = []\n",
    "\n",
    "    for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "        uids, baskets, lens = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, hidden)\n",
    "        for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "            scores = []\n",
    "            du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "            # calculating <u,p> score for all test items <u,p> pair\n",
    "            positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "            p_length = len(positives)\n",
    "            positives = torch.LongTensor(positives)\n",
    "\n",
    "            # Deal with positives samples\n",
    "            scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "            for s in scores_pos:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Deal with negative samples\n",
    "            negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "            negtives = torch.LongTensor(negtives)\n",
    "            scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "            for s in scores_neg:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Calculate hit-ratio\n",
    "            index_k = []\n",
    "            for k in range(Config().top_k):\n",
    "                index = scores.index(max(scores))\n",
    "                index_k.append(index)\n",
    "                scores[index] = -9999\n",
    "            single_hit = len((set(np.arange(0, p_length)) & set(index_k)))/p_length\n",
    "            results.append([uid,index_k, set(np.arange(0, p_length)), single_hit])\n",
    "            hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "            hitratio_denom += p_length\n",
    "\n",
    "            # Calculate NDCG\n",
    "            u_dcg = 0\n",
    "            u_idcg = 0\n",
    "            for k in range(Config().top_k):\n",
    "                if index_k[k] < p_length: \n",
    "                    u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "            ndcg += u_dcg / u_idcg\n",
    "\n",
    "    hitratio = hitratio_numer / hitratio_denom\n",
    "    ndcg = ndcg / len(train_data)\n",
    "    print('Hit ratio[{0}]: {1}'.format(Config().top_k, hitratio))\n",
    "    print('NDCG[{0}]: {1}'.format(Config().top_k, ndcg))\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results_ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results_, columns=['UserID', 'Prediction', 'Actual', 'Hit-Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confi_cut(score):\n",
    "    if score >= 0.0 and score <0.2:\n",
    "        return '0-0.2'\n",
    "    elif score >= 0.2 and score <0.4:\n",
    "        return '0.2-0.4'\n",
    "    elif score >= 0.4 and score <0.6:\n",
    "        return '0.4-0.6'\n",
    "    elif score >= 0.6 and score <0.8:\n",
    "        return '0.6-0.8'\n",
    "    else:\n",
    "        return '0.8-1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['confi'] = result_df['Hit-Ratio'].apply(lambda x: confi_cut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results/final_cluster_0_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
