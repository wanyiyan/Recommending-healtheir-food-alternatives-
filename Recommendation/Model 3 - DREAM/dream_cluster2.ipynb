{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to create a recommendation engine (DREAM) for cluster 2 from customer segmentation of instacart dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "   * Input: sequence of baskets of cluster 2 customers\n",
    "   * Negative sample : The food items which is never purchased by customers\n",
    "   \n",
    "### Evaluation Metric (Top 15 items)\n",
    "   * Hit Rate @15\n",
    "       - Counts the fraction of times that the ground truth next item is among the top 15 items.\n",
    "       - we only have one test item for each user, Hit@15 is equivalent to Recall@15\n",
    "       - It is also propotional to Precision@15\n",
    "   * NDCG@15\n",
    "       - A position aware metric with assigns larger weights on higher positions.\n",
    "       \n",
    "### Model \n",
    "   * Model is saved under runs folder with this key 1606154486\n",
    "   * This key 1606154486 is required to run it on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import data_helper as dh\n",
    "from configc3 import Config\n",
    "from rnn_model import DRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"✔︎ DREAM Model Training...\")\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/training-{0}.log\".format(time.asctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:torch-log:                                         MODEL_DIR|runs/                                             \n",
      "INFO:torch-log:                                       NEG_SAMPLES|../data/neg_sample_insta_all_sampled_b4.pickle    \n",
      "INFO:torch-log:                                       TESTSET_DIR|../data/allb4_cluster2_test.json                  \n",
      "INFO:torch-log:                                   TRAININGSET_DIR|../data/allb4_cluster2_train.json                 \n",
      "INFO:torch-log:                                 VALIDATIONSET_DIR|../data/allb4_cluster2_val.json                   \n",
      "INFO:torch-log:                                  BASKET_POOL_TYPE|max                                               \n",
      "INFO:torch-log:                                        BATCH_SIZE|250                                               \n",
      "INFO:torch-log:                                              CLIP|10                                                \n",
      "INFO:torch-log:                                              CUDA|1                                                 \n",
      "INFO:torch-log:                                           DROPOUT|0.5                                               \n",
      "INFO:torch-log:                                     EMBEDDING_DIM|32                                                \n",
      "INFO:torch-log:                                            EPOCHS|111                                               \n",
      "INFO:torch-log:                                     LEARNING_RATE|0.01                                              \n",
      "INFO:torch-log:                                      LOG_INTERVAL|1                                                 \n",
      "INFO:torch-log:                                           NEG_NUM|500                                               \n",
      "INFO:torch-log:                                       NUM_PRODUCT|58925                                             \n",
      "INFO:torch-log:                                     RNN_LAYER_NUM|2                                                 \n",
      "INFO:torch-log:                                          RNN_TYPE|LSTM                                              \n",
      "INFO:torch-log:                                           SEQ_LEN|99                                                \n",
      "INFO:torch-log:                                             TOP_K|15                                                \n",
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dilim = '-' * 120\n",
    "logger.info(dilim)\n",
    "for attr in sorted(Config().__dict__):\n",
    "    logger.info('{:>50}|{:<50}'.format(attr.upper(), Config().__dict__[attr]))\n",
    "logger.info(dilim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Validation data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n",
      "INFO:torch-log:Save into /home/reshmask/Next-Basket-Recommendation-master/DREAM/runs/1606154486\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:82: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     1 /    21 | ms/batch 221.80 | Loss 1.4574 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     2 /    21 | ms/batch 115.78 | Loss 0.6800 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     3 /    21 | ms/batch 117.11 | Loss 0.6606 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     4 /    21 | ms/batch 73.97 | Loss 0.6448 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     5 /    21 | ms/batch 98.89 | Loss 0.6366 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     6 /    21 | ms/batch 73.02 | Loss 0.6355 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     7 /    21 | ms/batch 84.41 | Loss 0.6272 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     8 /    21 | ms/batch 89.92 | Loss 0.5970 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     9 /    21 | ms/batch 69.94 | Loss 0.5848 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    10 /    21 | ms/batch 74.46 | Loss 0.5775 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    11 /    21 | ms/batch 88.16 | Loss 0.5952 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    12 /    21 | ms/batch 111.90 | Loss 0.5859 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    13 /    21 | ms/batch 74.16 | Loss 0.5659 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    14 /    21 | ms/batch 113.70 | Loss 0.5389 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    15 /    21 | ms/batch 102.45 | Loss 0.5611 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    16 /    21 | ms/batch 117.22 | Loss 0.5579 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    17 /    21 | ms/batch 97.56 | Loss 0.5368 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    18 /    21 | ms/batch 118.93 | Loss 0.5149 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    19 /    21 | ms/batch 90.72 | Loss 0.5166 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    20 /    21 | ms/batch 114.92 | Loss 0.5152 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:90: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:99: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Validation]| Epochs  99 | Elapsed 3541.67 | Loss 0.4950 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs  99 | Hit ratio 0.2190 | NDCG 0.1091 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     1 /    21 | ms/batch 203.41 | Loss 0.9779 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     2 /    21 | ms/batch 83.63 | Loss 0.4667 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     3 /    21 | ms/batch 67.50 | Loss 0.4551 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     4 /    21 | ms/batch 79.29 | Loss 0.4646 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     5 /    21 | ms/batch 83.18 | Loss 0.4349 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     6 /    21 | ms/batch 86.96 | Loss 0.4448 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     7 /    21 | ms/batch 90.12 | Loss 0.4434 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     8 /    21 | ms/batch 82.98 | Loss 0.4158 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     9 /    21 | ms/batch 84.11 | Loss 0.4228 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    10 /    21 | ms/batch 65.94 | Loss 0.3693 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    11 /    21 | ms/batch 76.88 | Loss 0.3890 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    12 /    21 | ms/batch 84.17 | Loss 0.3859 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    13 /    21 | ms/batch 88.95 | Loss 0.3829 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    14 /    21 | ms/batch 87.62 | Loss 0.3555 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    15 /    21 | ms/batch 83.30 | Loss 0.3598 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    16 /    21 | ms/batch 73.99 | Loss 0.3539 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    17 /    21 | ms/batch 85.94 | Loss 0.3263 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    18 /    21 | ms/batch 83.98 | Loss 0.3487 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    19 /    21 | ms/batch 79.10 | Loss 0.3190 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    20 /    21 | ms/batch 86.38 | Loss 0.3248 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 102 | Elapsed 3590.00 | Loss 0.3251 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 102 | Hit ratio 0.3693 | NDCG 0.1778 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     1 /    21 | ms/batch 157.96 | Loss 0.5852 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     2 /    21 | ms/batch 70.87 | Loss 0.2553 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     3 /    21 | ms/batch 87.66 | Loss 0.2926 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     4 /    21 | ms/batch 85.57 | Loss 0.2767 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     5 /    21 | ms/batch 62.61 | Loss 0.2724 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     6 /    21 | ms/batch 82.37 | Loss 0.2619 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     7 /    21 | ms/batch 86.05 | Loss 0.2635 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     8 /    21 | ms/batch 75.44 | Loss 0.2709 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     9 /    21 | ms/batch 86.28 | Loss 0.2538 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    10 /    21 | ms/batch 81.89 | Loss 0.2346 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    11 /    21 | ms/batch 90.99 | Loss 0.2317 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    12 /    21 | ms/batch 83.33 | Loss 0.2402 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    13 /    21 | ms/batch 66.06 | Loss 0.2183 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    14 /    21 | ms/batch 87.86 | Loss 0.2029 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    15 /    21 | ms/batch 88.50 | Loss 0.2314 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    16 /    21 | ms/batch 70.87 | Loss 0.2080 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    17 /    21 | ms/batch 85.40 | Loss 0.2004 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    18 /    21 | ms/batch 84.49 | Loss 0.2203 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    19 /    21 | ms/batch 88.23 | Loss 0.2085 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    20 /    21 | ms/batch 73.80 | Loss 0.2003 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Validation]| Epochs 104 | Elapsed 3601.67 | Loss 0.2331 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 104 | Hit ratio 0.4896 | NDCG 0.2313 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     1 /    21 | ms/batch 156.04 | Loss 0.3681 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     2 /    21 | ms/batch 73.20 | Loss 0.1721 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     3 /    21 | ms/batch 69.67 | Loss 0.1700 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     4 /    21 | ms/batch 91.58 | Loss 0.1550 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     5 /    21 | ms/batch 74.12 | Loss 0.1680 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     6 /    21 | ms/batch 72.17 | Loss 0.1553 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     7 /    21 | ms/batch 82.24 | Loss 0.1728 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     8 /    21 | ms/batch 90.27 | Loss 0.1653 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     9 /    21 | ms/batch 72.99 | Loss 0.1680 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    10 /    21 | ms/batch 80.93 | Loss 0.1794 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    11 /    21 | ms/batch 87.48 | Loss 0.1593 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    12 /    21 | ms/batch 86.28 | Loss 0.1526 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    13 /    21 | ms/batch 83.60 | Loss 0.1566 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    14 /    21 | ms/batch 88.42 | Loss 0.1444 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    15 /    21 | ms/batch 64.14 | Loss 0.1615 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    16 /    21 | ms/batch 87.35 | Loss 0.1669 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    17 /    21 | ms/batch 81.89 | Loss 0.1466 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    18 /    21 | ms/batch 83.66 | Loss 0.1566 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    19 /    21 | ms/batch 68.98 | Loss 0.1742 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    20 /    21 | ms/batch 68.69 | Loss 0.1533 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 106 | Elapsed 3596.67 | Loss 0.2016 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 106 | Hit ratio 0.5523 | NDCG 0.2559 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Validation data processing...\")\n",
    "    validation_data = dh.load_data(Config().VALIDATIONSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Model config\n",
    "    model = DRModel(Config())\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config().learning_rate)\n",
    "\n",
    "    def bpr_loss(uids, baskets, dynamic_user, item_embedding):\n",
    "        \"\"\"\n",
    "        Bayesian personalized ranking loss for implicit feedback.\n",
    "\n",
    "        Args:\n",
    "            uids: batch of users' ID\n",
    "            baskets: batch of users' baskets\n",
    "            dynamic_user: batch of users' dynamic representations\n",
    "            item_embedding: item_embedding matrix\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for uid, bks, du in zip(uids, baskets, dynamic_user):\n",
    "            du_p_product = torch.mm(du, item_embedding.t())  # shape: [pad_len, num_item]\n",
    "            loss_u = []  # loss for user\n",
    "            for t, basket_t in enumerate(bks):\n",
    "                if basket_t[0] != 0 and t != 0:\n",
    "                    pos_idx = torch.cuda.LongTensor(basket_t) \n",
    "\n",
    "                    # Sample negative products\n",
    "                    neg = random.sample(list(neg_samples[uid]), len(basket_t))\n",
    "                    neg_idx = torch.cuda.LongTensor(neg) \n",
    "\n",
    "                    # Score p(u, t, v > v')\n",
    "                    score = du_p_product[t - 1][pos_idx] - du_p_product[t - 1][neg_idx]\n",
    "\n",
    "                    # Average Negative log likelihood for basket_t\n",
    "                    loss_u.append(torch.mean(-torch.nn.LogSigmoid()(score)))\n",
    "            for i in loss_u:\n",
    "                loss = loss + i / len(loss_u)\n",
    "        avg_loss = torch.div(loss, len(baskets))\n",
    "        return avg_loss\n",
    "\n",
    "    def train_model():\n",
    "        model.train()  # turn on training mode for dropout\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        train_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(train_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=True)):\n",
    "            uids, baskets, lens = x\n",
    "            model.zero_grad()  \n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid gradient exploding\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config().clip)\n",
    "\n",
    "            # Parameter updating\n",
    "            optimizer.step()\n",
    "            train_loss += loss.data\n",
    "\n",
    "            # Logging\n",
    "            if i % Config().log_interval == 0 and i > 0:\n",
    "                elapsed = (time.clock() - start_time) / Config().log_interval\n",
    "                cur_loss = train_loss.item() / Config().log_interval  # turn tensor into float\n",
    "                train_loss = 0\n",
    "                start_time = time.clock()\n",
    "                logger.info('[Training]| Epochs {:3d} | Batch {:5d} / {:5d} | ms/batch {:02.2f} | Loss {:05.4f} |'\n",
    "                            .format(epoch, i, num_batches, elapsed, cur_loss))\n",
    "\n",
    "    def validate_model():\n",
    "        model.eval()\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        val_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(validation_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(validation_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            val_loss += loss.data\n",
    "\n",
    "        # Logging\n",
    "        elapsed = (time.clock() - start_time) * 1000 / num_batches\n",
    "        val_loss = val_loss.item() / num_batches\n",
    "        logger.info('[Validation]| Epochs {:3d} | Elapsed {:02.2f} | Loss {:05.4f} |'\n",
    "                    .format(epoch, elapsed, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def test_model():\n",
    "        model.eval()\n",
    "        item_embedding = model.encode.weight\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "\n",
    "        hitratio_numer = 0\n",
    "        hitratio_denom = 0\n",
    "        ndcg = 0.0\n",
    "\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "                scores = []\n",
    "                du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "                # calculating <u,p> score for all test items <u,p> pair\n",
    "                positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "                p_length = len(positives)\n",
    "                positives = torch.cuda.LongTensor(positives) \n",
    "\n",
    "                # Deal with positives samples\n",
    "                scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "                for s in scores_pos:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Deal with negative samples\n",
    "                negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "                negtives = torch.cuda.LongTensor(negtives)\n",
    "                scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "                for s in scores_neg:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Calculate hit-ratio\n",
    "                index_k = []\n",
    "                for k in range(Config().top_k):\n",
    "                    index = scores.index(max(scores))\n",
    "                    index_k.append(index)\n",
    "                    scores[index] = -9999\n",
    "                hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "                hitratio_denom += p_length\n",
    "\n",
    "                # Calculate NDCG\n",
    "                u_dcg = 0\n",
    "                u_idcg = 0\n",
    "                for k in range(Config().top_k):\n",
    "                    if index_k[k] < p_length: \n",
    "                        u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                    u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                ndcg += u_dcg / u_idcg\n",
    "\n",
    "        hit_ratio = hitratio_numer / hitratio_denom\n",
    "        ndcg = ndcg / len(train_data)\n",
    "        logger.info('[Test]| Epochs {:3d} | Hit ratio {:02.4f} | NDCG {:05.4f} |'\n",
    "                    .format(epoch, hit_ratio, ndcg))\n",
    "        return hit_ratio, ndcg\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    logger.info('Save into {0}'.format(out_dir))\n",
    "    checkpoint_dir = out_dir + '/model-{epoch:02d}-{hitratio:.4f}-{ndcg:.4f}.model'\n",
    "\n",
    "    best_hit_ratio = None\n",
    "\n",
    "    try:\n",
    "        # Training\n",
    "        for epoch in [99, 102, 104, 106]: #range(100, Config().epochs):\n",
    "            train_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            val_loss = validate_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            hit_ratio, ndcg = test_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            # Checkpoint\n",
    "            if not best_hit_ratio or hit_ratio > best_hit_ratio:\n",
    "                with open(checkpoint_dir.format(epoch=epoch, hitratio=hit_ratio, ndcg=ndcg), 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_hit_ratio = hit_ratio\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('*' * 89)\n",
    "        logger.info('Early Stopping!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☛ Please input the model file you want to test: 1606154486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ The format of your input is legal, now loading to next step...\n",
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio[15]: 0.21904607660539316\n",
      "NDCG[15]: 0.10937796104778102\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from configc3 import Config\n",
    "import data_helper as dh\n",
    "\n",
    "#key to use:1604813070\n",
    "\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/test-{0}.log\".format(time.asctime()))\n",
    "\n",
    "MODEL = input(\"☛ Please input the model file you want to test: \")\n",
    "\n",
    "while not (MODEL.isdigit() and len(MODEL) == 10):\n",
    "    MODEL = input(\"✘ The format of your input is illegal, it should be like(1490175368), please re-input: \")\n",
    "logger.info(\"✔︎ The format of your input is legal, now loading to next step...\")\n",
    "\n",
    "MODEL_DIR = dh.load_model_file(MODEL)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Load model\n",
    "    dr_model = torch.load(MODEL_DIR)\n",
    "\n",
    "    dr_model.eval()\n",
    "\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    hidden = dr_model.init_hidden(Config().batch_size)\n",
    "\n",
    "    hitratio_numer = 0\n",
    "    hitratio_denom = 0\n",
    "    ndcg = 0.0\n",
    "    results = []\n",
    "\n",
    "    for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "        uids, baskets, lens = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, hidden)\n",
    "        for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "            scores = []\n",
    "            du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "            # calculating <u,p> score for all test items <u,p> pair\n",
    "            positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "            p_length = len(positives)\n",
    "            positives = torch.LongTensor(positives)\n",
    "\n",
    "            # Deal with positives samples\n",
    "            scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "            for s in scores_pos:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Deal with negative samples\n",
    "            negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "            negtives = torch.LongTensor(negtives)\n",
    "            scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "            for s in scores_neg:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Calculate hit-ratio\n",
    "            index_k = []\n",
    "            for k in range(Config().top_k):\n",
    "                index = scores.index(max(scores))\n",
    "                index_k.append(index)\n",
    "                scores[index] = -9999\n",
    "            single_hit = len((set(np.arange(0, p_length)) & set(index_k)))/p_length\n",
    "            results.append([uid,index_k, set(np.arange(0, p_length)), single_hit])\n",
    "            hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "            hitratio_denom += p_length\n",
    "\n",
    "            # Calculate NDCG\n",
    "            u_dcg = 0\n",
    "            u_idcg = 0\n",
    "            for k in range(Config().top_k):\n",
    "                if index_k[k] < p_length:  \n",
    "                    u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "            ndcg += u_dcg / u_idcg\n",
    "\n",
    "    hitratio = hitratio_numer / hitratio_denom\n",
    "    ndcg = ndcg / len(train_data)\n",
    "    print('Hit ratio[{0}]: {1}'.format(Config().top_k, hitratio))\n",
    "    print('NDCG[{0}]: {1}'.format(Config().top_k, ndcg))\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results_ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results_, columns=['UserID', 'Prediction', 'Actual', 'Hit-Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confi_cut(score):\n",
    "    if score >= 0.0 and score <0.2:\n",
    "        return '0-0.2'\n",
    "    elif score >= 0.2 and score <0.4:\n",
    "        return '0.2-0.4'\n",
    "    elif score >= 0.4 and score <0.6:\n",
    "        return '0.4-0.6'\n",
    "    elif score >= 0.6 and score <0.8:\n",
    "        return '0.6-0.8'\n",
    "    else:\n",
    "        return '0.8-1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['confi'] = result_df['Hit-Ratio'].apply(lambda x: confi_cut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results/final_cluster_2_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
