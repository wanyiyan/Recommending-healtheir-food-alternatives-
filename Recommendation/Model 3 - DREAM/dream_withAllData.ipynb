{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to create a recommendation Model 3- DREAM for sequential basket dataset of instacart 2017 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author :  Reshma Patil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "   * Input: sequence of baskets\n",
    "   * Negative sample : The food items which is never purchased by customers\n",
    "   \n",
    "### Evaluation Metric (Top 15 items)\n",
    "   * Hit Rate @15\n",
    "       - Counts the fraction of times that the ground truth next item is among the top 15 items.\n",
    "       - we only have one test item for each user, Hit@15 is equivalent to Recall@15\n",
    "       - It is also propotional to Precision@15\n",
    "   * NDCG@15\n",
    "       - A position aware metric with assigns larger weights on higher positions.\n",
    "       \n",
    "### Model \n",
    "    * Model is saved under runs folder with this key 1605474009\n",
    "    * This key 1605474009 is required to run it on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import data_helper as dh\n",
    "from config import Config\n",
    "from rnn_model import DRModel\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"✔︎ DREAM Model Training...\")\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/training-{0}.log\".format(time.asctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:torch-log:                                         MODEL_DIR|runs/                                             \n",
      "INFO:torch-log:                                       NEG_SAMPLES|../data/neg_sample_insta_all_sampled_b4.pickle    \n",
      "INFO:torch-log:                                       TESTSET_DIR|../data/test_insta_all_b4.json                    \n",
      "INFO:torch-log:                                   TRAININGSET_DIR|../data/train_insta_all_b4.json                   \n",
      "INFO:torch-log:                                 VALIDATIONSET_DIR|../data/validation_insta_all_b4.json              \n",
      "INFO:torch-log:                                  BASKET_POOL_TYPE|max                                               \n",
      "INFO:torch-log:                                        BATCH_SIZE|1000                                              \n",
      "INFO:torch-log:                                              CLIP|10                                                \n",
      "INFO:torch-log:                                              CUDA|1                                                 \n",
      "INFO:torch-log:                                           DROPOUT|0.5                                               \n",
      "INFO:torch-log:                                     EMBEDDING_DIM|32                                                \n",
      "INFO:torch-log:                                            EPOCHS|111                                               \n",
      "INFO:torch-log:                                     LEARNING_RATE|0.01                                              \n",
      "INFO:torch-log:                                      LOG_INTERVAL|1                                                 \n",
      "INFO:torch-log:                                           NEG_NUM|500                                               \n",
      "INFO:torch-log:                                       NUM_PRODUCT|58925                                             \n",
      "INFO:torch-log:                                     RNN_LAYER_NUM|2                                                 \n",
      "INFO:torch-log:                                          RNN_TYPE|LSTM                                              \n",
      "INFO:torch-log:                                           SEQ_LEN|99                                                \n",
      "INFO:torch-log:                                             TOP_K|15                                                \n",
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dilim = '-' * 120\n",
    "logger.info(dilim)\n",
    "for attr in sorted(Config().__dict__):\n",
    "    logger.info('{:>50}|{:<50}'.format(attr.upper(), Config().__dict__[attr]))\n",
    "logger.info(dilim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Validation data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n",
      "INFO:torch-log:Save into /home/reshmask/Next-Basket-Recommendation-master/DREAM/runs/1605474009\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:82: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     1 /   157 | ms/batch 854.51 | Loss 1.4687 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     2 /   157 | ms/batch 524.27 | Loss 0.6940 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     3 /   157 | ms/batch 526.58 | Loss 0.6847 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     4 /   157 | ms/batch 537.58 | Loss 0.6768 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     5 /   157 | ms/batch 484.71 | Loss 0.6679 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     6 /   157 | ms/batch 477.83 | Loss 0.6669 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     7 /   157 | ms/batch 528.21 | Loss 0.6537 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     8 /   157 | ms/batch 533.63 | Loss 0.6512 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     9 /   157 | ms/batch 567.84 | Loss 0.6393 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    10 /   157 | ms/batch 547.25 | Loss 0.6290 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    11 /   157 | ms/batch 536.58 | Loss 0.6164 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    12 /   157 | ms/batch 582.35 | Loss 0.6085 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    13 /   157 | ms/batch 463.08 | Loss 0.5970 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    14 /   157 | ms/batch 534.69 | Loss 0.5822 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    15 /   157 | ms/batch 538.32 | Loss 0.5703 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    16 /   157 | ms/batch 521.30 | Loss 0.5547 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    17 /   157 | ms/batch 536.12 | Loss 0.5400 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    18 /   157 | ms/batch 527.49 | Loss 0.5238 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    19 /   157 | ms/batch 561.28 | Loss 0.5180 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    20 /   157 | ms/batch 530.16 | Loss 0.5082 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    21 /   157 | ms/batch 536.70 | Loss 0.4794 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    22 /   157 | ms/batch 534.29 | Loss 0.4686 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    23 /   157 | ms/batch 523.08 | Loss 0.4573 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    24 /   157 | ms/batch 539.09 | Loss 0.4453 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    25 /   157 | ms/batch 510.52 | Loss 0.4226 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    26 /   157 | ms/batch 477.44 | Loss 0.4153 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    27 /   157 | ms/batch 450.21 | Loss 0.4085 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    28 /   157 | ms/batch 454.20 | Loss 0.3881 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    29 /   157 | ms/batch 475.35 | Loss 0.3745 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    30 /   157 | ms/batch 465.99 | Loss 0.3574 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    31 /   157 | ms/batch 463.18 | Loss 0.3600 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    32 /   157 | ms/batch 468.75 | Loss 0.3499 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    33 /   157 | ms/batch 470.55 | Loss 0.3290 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    34 /   157 | ms/batch 451.81 | Loss 0.3194 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    35 /   157 | ms/batch 451.11 | Loss 0.3229 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    36 /   157 | ms/batch 463.96 | Loss 0.3008 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    37 /   157 | ms/batch 455.01 | Loss 0.3014 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    38 /   157 | ms/batch 474.52 | Loss 0.2940 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    39 /   157 | ms/batch 448.00 | Loss 0.2799 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    40 /   157 | ms/batch 453.13 | Loss 0.2777 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    41 /   157 | ms/batch 457.12 | Loss 0.2728 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    42 /   157 | ms/batch 463.81 | Loss 0.2702 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    43 /   157 | ms/batch 463.59 | Loss 0.2578 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    44 /   157 | ms/batch 464.70 | Loss 0.2506 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    45 /   157 | ms/batch 458.95 | Loss 0.2442 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    46 /   157 | ms/batch 452.55 | Loss 0.2438 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    47 /   157 | ms/batch 477.78 | Loss 0.2411 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    48 /   157 | ms/batch 497.27 | Loss 0.2373 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    49 /   157 | ms/batch 449.89 | Loss 0.2296 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    50 /   157 | ms/batch 457.56 | Loss 0.2347 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    51 /   157 | ms/batch 470.50 | Loss 0.2295 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    52 /   157 | ms/batch 453.04 | Loss 0.2258 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    53 /   157 | ms/batch 456.96 | Loss 0.2302 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    54 /   157 | ms/batch 471.44 | Loss 0.2283 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    55 /   157 | ms/batch 492.23 | Loss 0.2242 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    56 /   157 | ms/batch 460.32 | Loss 0.2183 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    57 /   157 | ms/batch 465.72 | Loss 0.2193 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    58 /   157 | ms/batch 474.19 | Loss 0.2110 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    59 /   157 | ms/batch 480.33 | Loss 0.2111 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    60 /   157 | ms/batch 452.86 | Loss 0.2114 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    61 /   157 | ms/batch 456.54 | Loss 0.2133 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    62 /   157 | ms/batch 482.23 | Loss 0.2107 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    63 /   157 | ms/batch 455.58 | Loss 0.2146 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    64 /   157 | ms/batch 487.48 | Loss 0.1971 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    65 /   157 | ms/batch 453.09 | Loss 0.2002 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    66 /   157 | ms/batch 474.83 | Loss 0.2097 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    67 /   157 | ms/batch 468.01 | Loss 0.2061 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    68 /   157 | ms/batch 494.01 | Loss 0.1983 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    69 /   157 | ms/batch 448.93 | Loss 0.2039 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    70 /   157 | ms/batch 454.74 | Loss 0.1981 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    71 /   157 | ms/batch 467.24 | Loss 0.2086 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    72 /   157 | ms/batch 472.07 | Loss 0.2020 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    73 /   157 | ms/batch 468.87 | Loss 0.2031 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    74 /   157 | ms/batch 468.74 | Loss 0.2126 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    75 /   157 | ms/batch 471.07 | Loss 0.2007 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    76 /   157 | ms/batch 463.44 | Loss 0.1976 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 102 | Batch    77 /   157 | ms/batch 472.27 | Loss 0.1895 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    78 /   157 | ms/batch 460.45 | Loss 0.1941 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    79 /   157 | ms/batch 480.62 | Loss 0.1958 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    80 /   157 | ms/batch 459.72 | Loss 0.1959 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    81 /   157 | ms/batch 461.50 | Loss 0.1906 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    82 /   157 | ms/batch 456.97 | Loss 0.1981 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    83 /   157 | ms/batch 455.64 | Loss 0.1989 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    84 /   157 | ms/batch 466.23 | Loss 0.1983 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    85 /   157 | ms/batch 454.86 | Loss 0.1943 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    86 /   157 | ms/batch 504.92 | Loss 0.1910 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    87 /   157 | ms/batch 489.92 | Loss 0.1976 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    88 /   157 | ms/batch 487.42 | Loss 0.1975 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    89 /   157 | ms/batch 433.55 | Loss 0.1916 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    90 /   157 | ms/batch 450.89 | Loss 0.2019 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    91 /   157 | ms/batch 451.70 | Loss 0.1937 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    92 /   157 | ms/batch 454.24 | Loss 0.1870 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    93 /   157 | ms/batch 487.18 | Loss 0.1871 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    94 /   157 | ms/batch 463.75 | Loss 0.1857 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    95 /   157 | ms/batch 444.14 | Loss 0.1920 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    96 /   157 | ms/batch 479.31 | Loss 0.1896 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    97 /   157 | ms/batch 474.44 | Loss 0.1851 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    98 /   157 | ms/batch 446.35 | Loss 0.1964 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    99 /   157 | ms/batch 458.14 | Loss 0.1925 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   100 /   157 | ms/batch 466.71 | Loss 0.1930 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   101 /   157 | ms/batch 432.16 | Loss 0.1923 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   102 /   157 | ms/batch 469.41 | Loss 0.1902 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   103 /   157 | ms/batch 488.55 | Loss 0.1854 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   104 /   157 | ms/batch 457.59 | Loss 0.1844 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   105 /   157 | ms/batch 448.16 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   106 /   157 | ms/batch 476.55 | Loss 0.1849 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   107 /   157 | ms/batch 475.55 | Loss 0.1848 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   108 /   157 | ms/batch 475.09 | Loss 0.1828 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   109 /   157 | ms/batch 458.06 | Loss 0.1924 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   110 /   157 | ms/batch 475.52 | Loss 0.1892 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   111 /   157 | ms/batch 464.43 | Loss 0.1839 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   112 /   157 | ms/batch 452.02 | Loss 0.1881 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   113 /   157 | ms/batch 469.84 | Loss 0.1845 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   114 /   157 | ms/batch 481.94 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   115 /   157 | ms/batch 471.12 | Loss 0.1877 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   116 /   157 | ms/batch 454.07 | Loss 0.1875 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   117 /   157 | ms/batch 449.18 | Loss 0.1838 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   118 /   157 | ms/batch 453.80 | Loss 0.1881 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   119 /   157 | ms/batch 476.12 | Loss 0.1871 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   120 /   157 | ms/batch 463.39 | Loss 0.1903 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   121 /   157 | ms/batch 480.16 | Loss 0.1829 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   122 /   157 | ms/batch 460.95 | Loss 0.1804 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   123 /   157 | ms/batch 433.80 | Loss 0.1785 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   124 /   157 | ms/batch 457.93 | Loss 0.1841 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   125 /   157 | ms/batch 467.61 | Loss 0.1808 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   126 /   157 | ms/batch 465.62 | Loss 0.1829 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   127 /   157 | ms/batch 462.77 | Loss 0.1831 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   128 /   157 | ms/batch 464.87 | Loss 0.1792 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   129 /   157 | ms/batch 456.66 | Loss 0.1822 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   130 /   157 | ms/batch 458.56 | Loss 0.1941 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   131 /   157 | ms/batch 462.48 | Loss 0.1910 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   132 /   157 | ms/batch 457.12 | Loss 0.1836 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   133 /   157 | ms/batch 463.23 | Loss 0.1762 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   134 /   157 | ms/batch 469.07 | Loss 0.1914 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   135 /   157 | ms/batch 463.15 | Loss 0.1949 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   136 /   157 | ms/batch 467.72 | Loss 0.1846 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   137 /   157 | ms/batch 463.56 | Loss 0.1876 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   138 /   157 | ms/batch 459.84 | Loss 0.1804 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   139 /   157 | ms/batch 472.36 | Loss 0.1852 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   140 /   157 | ms/batch 442.88 | Loss 0.1835 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   141 /   157 | ms/batch 483.38 | Loss 0.1751 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   142 /   157 | ms/batch 480.60 | Loss 0.1834 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   143 /   157 | ms/batch 467.05 | Loss 0.1896 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   144 /   157 | ms/batch 467.09 | Loss 0.1798 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   145 /   157 | ms/batch 457.24 | Loss 0.1813 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   146 /   157 | ms/batch 468.86 | Loss 0.1849 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   147 /   157 | ms/batch 434.78 | Loss 0.1809 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   148 /   157 | ms/batch 503.21 | Loss 0.1811 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   149 /   157 | ms/batch 456.70 | Loss 0.1876 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   150 /   157 | ms/batch 466.39 | Loss 0.1833 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   151 /   157 | ms/batch 468.61 | Loss 0.1807 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   152 /   157 | ms/batch 461.78 | Loss 0.1836 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   153 /   157 | ms/batch 453.94 | Loss 0.1817 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   154 /   157 | ms/batch 471.89 | Loss 0.1861 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   155 /   157 | ms/batch 450.23 | Loss 0.1895 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch   156 /   157 | ms/batch 468.49 | Loss 0.1804 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:90: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:99: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Validation]| Epochs 102 | Elapsed 16475.00 | Loss 0.1805 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 102 | Hit ratio 0.4583 | NDCG 0.3552 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     1 /   157 | ms/batch 823.68 | Loss 0.3504 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     2 /   157 | ms/batch 429.21 | Loss 0.1769 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     3 /   157 | ms/batch 427.74 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     4 /   157 | ms/batch 447.88 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     5 /   157 | ms/batch 431.19 | Loss 0.1853 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     6 /   157 | ms/batch 436.53 | Loss 0.1793 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     7 /   157 | ms/batch 446.57 | Loss 0.1745 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     8 /   157 | ms/batch 456.61 | Loss 0.1759 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     9 /   157 | ms/batch 426.78 | Loss 0.1758 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    10 /   157 | ms/batch 441.40 | Loss 0.1776 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    11 /   157 | ms/batch 435.02 | Loss 0.1767 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    12 /   157 | ms/batch 424.33 | Loss 0.1859 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    13 /   157 | ms/batch 414.42 | Loss 0.1736 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    14 /   157 | ms/batch 432.42 | Loss 0.1790 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    15 /   157 | ms/batch 415.08 | Loss 0.1836 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    16 /   157 | ms/batch 432.05 | Loss 0.1833 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    17 /   157 | ms/batch 434.17 | Loss 0.1897 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    18 /   157 | ms/batch 444.92 | Loss 0.1833 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    19 /   157 | ms/batch 433.69 | Loss 0.1852 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    20 /   157 | ms/batch 443.37 | Loss 0.1775 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    21 /   157 | ms/batch 435.11 | Loss 0.1781 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    22 /   157 | ms/batch 438.26 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    23 /   157 | ms/batch 424.14 | Loss 0.1809 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    24 /   157 | ms/batch 428.14 | Loss 0.1806 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    25 /   157 | ms/batch 452.53 | Loss 0.1812 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    26 /   157 | ms/batch 408.25 | Loss 0.1743 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    27 /   157 | ms/batch 416.88 | Loss 0.1774 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    28 /   157 | ms/batch 434.65 | Loss 0.1786 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    29 /   157 | ms/batch 404.81 | Loss 0.1875 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    30 /   157 | ms/batch 442.25 | Loss 0.1711 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    31 /   157 | ms/batch 429.06 | Loss 0.1763 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    32 /   157 | ms/batch 441.09 | Loss 0.1829 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    33 /   157 | ms/batch 417.85 | Loss 0.1821 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    34 /   157 | ms/batch 431.99 | Loss 0.1720 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    35 /   157 | ms/batch 424.82 | Loss 0.1843 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    36 /   157 | ms/batch 435.65 | Loss 0.1783 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    37 /   157 | ms/batch 441.21 | Loss 0.1764 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    38 /   157 | ms/batch 441.92 | Loss 0.1725 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    39 /   157 | ms/batch 441.14 | Loss 0.1785 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    40 /   157 | ms/batch 428.14 | Loss 0.1822 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    41 /   157 | ms/batch 425.26 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    42 /   157 | ms/batch 431.12 | Loss 0.1810 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    43 /   157 | ms/batch 432.64 | Loss 0.1797 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    44 /   157 | ms/batch 431.90 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    45 /   157 | ms/batch 420.66 | Loss 0.1848 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    46 /   157 | ms/batch 430.12 | Loss 0.1843 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    47 /   157 | ms/batch 447.97 | Loss 0.1767 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    48 /   157 | ms/batch 425.92 | Loss 0.1761 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    49 /   157 | ms/batch 443.54 | Loss 0.1765 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    50 /   157 | ms/batch 434.30 | Loss 0.1775 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    51 /   157 | ms/batch 435.61 | Loss 0.1768 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    52 /   157 | ms/batch 438.00 | Loss 0.1878 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    53 /   157 | ms/batch 435.29 | Loss 0.1767 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    54 /   157 | ms/batch 414.50 | Loss 0.1799 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    55 /   157 | ms/batch 435.23 | Loss 0.1745 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    56 /   157 | ms/batch 440.72 | Loss 0.1803 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    57 /   157 | ms/batch 429.85 | Loss 0.1762 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    58 /   157 | ms/batch 409.55 | Loss 0.1720 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    59 /   157 | ms/batch 451.92 | Loss 0.1830 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    60 /   157 | ms/batch 439.62 | Loss 0.1778 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    61 /   157 | ms/batch 432.73 | Loss 0.1793 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    62 /   157 | ms/batch 439.60 | Loss 0.1785 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    63 /   157 | ms/batch 421.32 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    64 /   157 | ms/batch 416.77 | Loss 0.1841 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    65 /   157 | ms/batch 416.06 | Loss 0.1916 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    66 /   157 | ms/batch 433.39 | Loss 0.1735 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    67 /   157 | ms/batch 407.07 | Loss 0.1825 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    68 /   157 | ms/batch 436.77 | Loss 0.1717 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    69 /   157 | ms/batch 431.08 | Loss 0.1766 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    70 /   157 | ms/batch 431.05 | Loss 0.1832 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    71 /   157 | ms/batch 411.58 | Loss 0.1783 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    72 /   157 | ms/batch 447.80 | Loss 0.1738 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    73 /   157 | ms/batch 428.41 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    74 /   157 | ms/batch 416.75 | Loss 0.1868 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    75 /   157 | ms/batch 453.14 | Loss 0.1820 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    76 /   157 | ms/batch 430.22 | Loss 0.1771 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    77 /   157 | ms/batch 440.31 | Loss 0.1731 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    78 /   157 | ms/batch 434.76 | Loss 0.1742 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    79 /   157 | ms/batch 431.79 | Loss 0.1734 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    80 /   157 | ms/batch 422.21 | Loss 0.1806 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    81 /   157 | ms/batch 443.78 | Loss 0.1762 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    82 /   157 | ms/batch 427.11 | Loss 0.1793 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    83 /   157 | ms/batch 433.41 | Loss 0.1857 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    84 /   157 | ms/batch 431.22 | Loss 0.1776 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    85 /   157 | ms/batch 428.04 | Loss 0.1827 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 104 | Batch    86 /   157 | ms/batch 428.13 | Loss 0.1716 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    87 /   157 | ms/batch 419.90 | Loss 0.1723 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    88 /   157 | ms/batch 419.68 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    89 /   157 | ms/batch 417.26 | Loss 0.1776 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    90 /   157 | ms/batch 425.96 | Loss 0.1765 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    91 /   157 | ms/batch 413.96 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    92 /   157 | ms/batch 434.49 | Loss 0.1775 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    93 /   157 | ms/batch 443.13 | Loss 0.1792 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    94 /   157 | ms/batch 440.98 | Loss 0.1774 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    95 /   157 | ms/batch 436.48 | Loss 0.1771 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    96 /   157 | ms/batch 431.44 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    97 /   157 | ms/batch 428.54 | Loss 0.1778 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    98 /   157 | ms/batch 422.80 | Loss 0.1761 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    99 /   157 | ms/batch 423.84 | Loss 0.1758 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   100 /   157 | ms/batch 426.97 | Loss 0.1772 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   101 /   157 | ms/batch 429.99 | Loss 0.1815 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   102 /   157 | ms/batch 432.00 | Loss 0.1733 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   103 /   157 | ms/batch 436.50 | Loss 0.1756 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   104 /   157 | ms/batch 442.24 | Loss 0.1787 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   105 /   157 | ms/batch 426.76 | Loss 0.1733 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   106 /   157 | ms/batch 439.24 | Loss 0.1700 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   107 /   157 | ms/batch 426.88 | Loss 0.1755 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   108 /   157 | ms/batch 422.78 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   109 /   157 | ms/batch 415.59 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   110 /   157 | ms/batch 446.26 | Loss 0.1773 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   111 /   157 | ms/batch 432.80 | Loss 0.1766 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   112 /   157 | ms/batch 422.22 | Loss 0.1753 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   113 /   157 | ms/batch 427.36 | Loss 0.1772 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   114 /   157 | ms/batch 432.58 | Loss 0.1692 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   115 /   157 | ms/batch 447.27 | Loss 0.1784 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   116 /   157 | ms/batch 430.33 | Loss 0.1694 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   117 /   157 | ms/batch 434.05 | Loss 0.1715 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   118 /   157 | ms/batch 426.83 | Loss 0.1701 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   119 /   157 | ms/batch 437.15 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   120 /   157 | ms/batch 445.68 | Loss 0.1710 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   121 /   157 | ms/batch 433.86 | Loss 0.1737 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   122 /   157 | ms/batch 426.69 | Loss 0.1776 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   123 /   157 | ms/batch 433.86 | Loss 0.1699 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   124 /   157 | ms/batch 431.83 | Loss 0.1731 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   125 /   157 | ms/batch 436.89 | Loss 0.1731 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   126 /   157 | ms/batch 415.31 | Loss 0.1780 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   127 /   157 | ms/batch 424.49 | Loss 0.1675 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   128 /   157 | ms/batch 437.45 | Loss 0.1759 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   129 /   157 | ms/batch 438.22 | Loss 0.1793 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   130 /   157 | ms/batch 419.17 | Loss 0.1762 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   131 /   157 | ms/batch 402.68 | Loss 0.1725 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   132 /   157 | ms/batch 434.93 | Loss 0.1738 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   133 /   157 | ms/batch 420.85 | Loss 0.1735 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   134 /   157 | ms/batch 427.71 | Loss 0.1729 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   135 /   157 | ms/batch 408.95 | Loss 0.1741 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   136 /   157 | ms/batch 427.77 | Loss 0.1693 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   137 /   157 | ms/batch 415.71 | Loss 0.1703 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   138 /   157 | ms/batch 430.96 | Loss 0.1742 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   139 /   157 | ms/batch 419.90 | Loss 0.1715 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   140 /   157 | ms/batch 440.44 | Loss 0.1827 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   141 /   157 | ms/batch 444.83 | Loss 0.1717 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   142 /   157 | ms/batch 432.07 | Loss 0.1671 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   143 /   157 | ms/batch 446.61 | Loss 0.1609 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   144 /   157 | ms/batch 424.90 | Loss 0.1631 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   145 /   157 | ms/batch 429.85 | Loss 0.1668 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   146 /   157 | ms/batch 433.89 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   147 /   157 | ms/batch 447.88 | Loss 0.1752 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   148 /   157 | ms/batch 407.35 | Loss 0.1714 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   149 /   157 | ms/batch 426.80 | Loss 0.1689 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   150 /   157 | ms/batch 436.93 | Loss 0.1765 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   151 /   157 | ms/batch 433.46 | Loss 0.1684 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   152 /   157 | ms/batch 433.98 | Loss 0.1675 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   153 /   157 | ms/batch 438.19 | Loss 0.1592 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   154 /   157 | ms/batch 437.57 | Loss 0.1659 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   155 /   157 | ms/batch 424.87 | Loss 0.1645 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch   156 /   157 | ms/batch 433.88 | Loss 0.1731 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 104 | Elapsed 16550.25 | Loss 0.1649 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 104 | Hit ratio 0.4788 | NDCG 0.3686 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     1 /   157 | ms/batch 827.94 | Loss 0.3343 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     2 /   157 | ms/batch 435.81 | Loss 0.1567 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     3 /   157 | ms/batch 434.15 | Loss 0.1567 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     4 /   157 | ms/batch 414.68 | Loss 0.1651 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     5 /   157 | ms/batch 427.34 | Loss 0.1583 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     6 /   157 | ms/batch 441.54 | Loss 0.1632 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     7 /   157 | ms/batch 428.15 | Loss 0.1492 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     8 /   157 | ms/batch 422.04 | Loss 0.1645 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     9 /   157 | ms/batch 427.97 | Loss 0.1626 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    10 /   157 | ms/batch 418.91 | Loss 0.1552 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    11 /   157 | ms/batch 432.85 | Loss 0.1594 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    12 /   157 | ms/batch 425.23 | Loss 0.1607 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 106 | Batch    13 /   157 | ms/batch 430.27 | Loss 0.1598 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    14 /   157 | ms/batch 431.73 | Loss 0.1622 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    15 /   157 | ms/batch 425.48 | Loss 0.1632 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    16 /   157 | ms/batch 429.05 | Loss 0.1537 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    17 /   157 | ms/batch 427.56 | Loss 0.1587 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    18 /   157 | ms/batch 435.65 | Loss 0.1617 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    19 /   157 | ms/batch 420.58 | Loss 0.1605 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    20 /   157 | ms/batch 427.79 | Loss 0.1529 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    21 /   157 | ms/batch 409.54 | Loss 0.1590 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    22 /   157 | ms/batch 432.42 | Loss 0.1620 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    23 /   157 | ms/batch 432.43 | Loss 0.1635 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    24 /   157 | ms/batch 427.48 | Loss 0.1601 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    25 /   157 | ms/batch 398.50 | Loss 0.1626 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    26 /   157 | ms/batch 433.60 | Loss 0.1594 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    27 /   157 | ms/batch 431.55 | Loss 0.1561 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    28 /   157 | ms/batch 425.86 | Loss 0.1570 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    29 /   157 | ms/batch 441.92 | Loss 0.1543 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    30 /   157 | ms/batch 429.80 | Loss 0.1663 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    31 /   157 | ms/batch 411.77 | Loss 0.1553 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    32 /   157 | ms/batch 407.48 | Loss 0.1536 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    33 /   157 | ms/batch 434.64 | Loss 0.1581 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    34 /   157 | ms/batch 413.11 | Loss 0.1590 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    35 /   157 | ms/batch 452.43 | Loss 0.1517 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    36 /   157 | ms/batch 420.10 | Loss 0.1536 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    37 /   157 | ms/batch 416.97 | Loss 0.1494 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    38 /   157 | ms/batch 427.97 | Loss 0.1531 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    39 /   157 | ms/batch 430.74 | Loss 0.1530 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    40 /   157 | ms/batch 434.65 | Loss 0.1628 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    41 /   157 | ms/batch 431.02 | Loss 0.1470 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    42 /   157 | ms/batch 455.36 | Loss 0.1514 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    43 /   157 | ms/batch 446.70 | Loss 0.1508 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    44 /   157 | ms/batch 438.41 | Loss 0.1481 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    45 /   157 | ms/batch 424.64 | Loss 0.1582 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    46 /   157 | ms/batch 450.46 | Loss 0.1543 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    47 /   157 | ms/batch 429.71 | Loss 0.1537 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    48 /   157 | ms/batch 426.78 | Loss 0.1561 |\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☛ Please input the model file you want to test: 1605474009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ The format of your input is legal, now loading to next step...\n",
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit ratio[15]: 0.4787466836172534\n",
      "NDCG[15]: 0.3686306380642009\n"
     ]
    }
   ],
   "source": [
    "#key to use:1605474009\n",
    "\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/test-{0}.log\".format(time.asctime()))\n",
    "\n",
    "MODEL = input(\"☛ Please input the model file you want to test: \")\n",
    "\n",
    "while not (MODEL.isdigit() and len(MODEL) == 10):\n",
    "    MODEL = input(\"✘ The format of your input is illegal, it should be like(1490175368), please re-input: \")\n",
    "logger.info(\"✔︎ The format of your input is legal, now loading to next step...\")\n",
    "\n",
    "MODEL_DIR = dh.load_model_file(MODEL)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Load model\n",
    "    dr_model = torch.load(MODEL_DIR)\n",
    "\n",
    "    dr_model.eval()\n",
    "\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    hidden = dr_model.init_hidden(Config().batch_size)\n",
    "\n",
    "    hitratio_numer = 0\n",
    "    hitratio_denom = 0\n",
    "    ndcg = 0.0\n",
    "    results = []\n",
    "\n",
    "    for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "        uids, baskets, lens = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, hidden)\n",
    "        for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "            scores = []\n",
    "            du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "            # calculating <u,p> score for all test items <u,p> pair\n",
    "            positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "            p_length = len(positives)\n",
    "            positives = torch.LongTensor(positives)\n",
    "\n",
    "            # Deal with positives samples\n",
    "            scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "            for s in scores_pos:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Deal with negative samples\n",
    "            negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "            negtives = torch.LongTensor(negtives)\n",
    "            scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "            for s in scores_neg:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Calculate hit-ratio\n",
    "            index_k = []\n",
    "            for k in range(Config().top_k):\n",
    "                index = scores.index(max(scores))\n",
    "                index_k.append(index)\n",
    "                scores[index] = -9999\n",
    "            single_hit = len((set(np.arange(0, p_length)) & set(index_k)))/p_length\n",
    "            results.append([uid,index_k, set(np.arange(0, p_length)), single_hit])\n",
    "            hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "            hitratio_denom += p_length\n",
    "\n",
    "            # Calculate NDCG\n",
    "            u_dcg = 0\n",
    "            u_idcg = 0\n",
    "            for k in range(Config().top_k):\n",
    "                if index_k[k] < p_length:  # p_length \n",
    "                    u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "            ndcg += u_dcg / u_idcg\n",
    "\n",
    "    hitratio = hitratio_numer / hitratio_denom\n",
    "    ndcg = ndcg / len(train_data)\n",
    "    print('Hit ratio[{0}]: {1}'.format(Config().top_k, hitratio))\n",
    "    print('NDCG[{0}]: {1}'.format(Config().top_k, ndcg))\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results_ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results_, columns=['UserID', 'Prediction', 'Actual', 'Hit-Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confi_cut(score):\n",
    "    if score >= 0.0 and score <0.2:\n",
    "        return '0-0.2'\n",
    "    elif score >= 0.2 and score <0.4:\n",
    "        return '0.2-0.4'\n",
    "    elif score >= 0.4 and score <0.6:\n",
    "        return '0.4-0.6'\n",
    "    elif score >= 0.6 and score <0.8:\n",
    "        return '0.6-0.8'\n",
    "    else:\n",
    "        return '0.8-1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['confi'] = result_df['Hit-Ratio'].apply(lambda x: confi_cut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4-0.6    48859\n",
       "0.6-0.8    33118\n",
       "0.2-0.4    32967\n",
       "0.8-1.0    28707\n",
       "0-0.2      13349\n",
       "Name: confi, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df['confi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4-0.6    0.311204\n",
       "0.6-0.8    0.210943\n",
       "0.2-0.4    0.209981\n",
       "0.8-1.0    0.182847\n",
       "0-0.2      0.085025\n",
       "Name: confi, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df['confi'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results/final_all_data_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
