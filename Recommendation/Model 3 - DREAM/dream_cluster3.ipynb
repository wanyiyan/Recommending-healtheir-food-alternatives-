{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to create a recommendation engine (DREAM) for cluster 3 from customer segmentation of instacart dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "   * Input: sequence of baskets of cluster 3 customers\n",
    "   * Negative sample : The food items which is never purchased by customers\n",
    "   \n",
    "### Evaluation Metric (Top 15 items)\n",
    "   * Hit Rate @15\n",
    "       - Counts the fraction of times that the ground truth next item is among the top 15 items.\n",
    "       - we only have one test item for each user, Hit@15 is equivalent to Recall@15\n",
    "       - It is also propotional to Precision@15\n",
    "   * NDCG@15\n",
    "       - A position aware metric with assigns larger weights on higher positions.\n",
    "       \n",
    "### Model \n",
    "   * Model is saved under runs folder with this key 1605856919\n",
    "   * This key 1605856919 is required to run it on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import data_helper as dh\n",
    "from configc4 import Config\n",
    "from rnn_model import DRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"✔︎ DREAM Model Training...\")\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/training-{0}.log\".format(time.asctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:torch-log:                                         MODEL_DIR|runs/                                             \n",
      "INFO:torch-log:                                       NEG_SAMPLES|../data/neg_sample_insta_all_sampled_b4.pickle    \n",
      "INFO:torch-log:                                       TESTSET_DIR|../data/allb4_cluster3_test.json                  \n",
      "INFO:torch-log:                                   TRAININGSET_DIR|../data/allb4_cluster3_train.json                 \n",
      "INFO:torch-log:                                 VALIDATIONSET_DIR|../data/allb4_cluster3_val.json                   \n",
      "INFO:torch-log:                                  BASKET_POOL_TYPE|max                                               \n",
      "INFO:torch-log:                                        BATCH_SIZE|1000                                              \n",
      "INFO:torch-log:                                              CLIP|10                                                \n",
      "INFO:torch-log:                                              CUDA|1                                                 \n",
      "INFO:torch-log:                                           DROPOUT|0.5                                               \n",
      "INFO:torch-log:                                     EMBEDDING_DIM|32                                                \n",
      "INFO:torch-log:                                            EPOCHS|111                                               \n",
      "INFO:torch-log:                                     LEARNING_RATE|0.01                                              \n",
      "INFO:torch-log:                                      LOG_INTERVAL|1                                                 \n",
      "INFO:torch-log:                                           NEG_NUM|500                                               \n",
      "INFO:torch-log:                                       NUM_PRODUCT|58925                                             \n",
      "INFO:torch-log:                                     RNN_LAYER_NUM|2                                                 \n",
      "INFO:torch-log:                                          RNN_TYPE|LSTM                                              \n",
      "INFO:torch-log:                                           SEQ_LEN|99                                                \n",
      "INFO:torch-log:                                             TOP_K|15                                                \n",
      "INFO:torch-log:------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dilim = '-' * 120\n",
    "logger.info(dilim)\n",
    "for attr in sorted(Config().__dict__):\n",
    "    logger.info('{:>50}|{:<50}'.format(attr.upper(), Config().__dict__[attr]))\n",
    "logger.info(dilim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:✔︎ Loading data...\n",
      "INFO:torch-log:✔︎ Training data processing...\n",
      "INFO:torch-log:✔︎ Validation data processing...\n",
      "INFO:torch-log:✔︎ Test data processing...\n",
      "INFO:torch-log:✔︎ Load negative sample...\n",
      "INFO:torch-log:Save into /home/reshmask/Next-Basket-Recommendation-master/DREAM/runs/1605856919\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:79: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:82: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     1 /    58 | ms/batch 694.14 | Loss 1.4557 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     2 /    58 | ms/batch 254.75 | Loss 0.6951 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     3 /    58 | ms/batch 255.57 | Loss 0.6877 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     4 /    58 | ms/batch 286.13 | Loss 0.6795 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     5 /    58 | ms/batch 310.56 | Loss 0.6740 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     6 /    58 | ms/batch 332.56 | Loss 0.6710 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     7 /    58 | ms/batch 288.41 | Loss 0.6621 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     8 /    58 | ms/batch 306.70 | Loss 0.6563 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch     9 /    58 | ms/batch 364.51 | Loss 0.6513 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    10 /    58 | ms/batch 264.13 | Loss 0.6414 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    11 /    58 | ms/batch 351.37 | Loss 0.6297 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    12 /    58 | ms/batch 340.09 | Loss 0.6230 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    13 /    58 | ms/batch 298.10 | Loss 0.6097 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    14 /    58 | ms/batch 326.54 | Loss 0.5985 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    15 /    58 | ms/batch 335.62 | Loss 0.5804 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    16 /    58 | ms/batch 349.88 | Loss 0.5736 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    17 /    58 | ms/batch 263.45 | Loss 0.5536 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    18 /    58 | ms/batch 269.06 | Loss 0.5378 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    19 /    58 | ms/batch 339.04 | Loss 0.5261 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    20 /    58 | ms/batch 339.56 | Loss 0.5059 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    21 /    58 | ms/batch 335.42 | Loss 0.4984 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    22 /    58 | ms/batch 302.98 | Loss 0.4885 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    23 /    58 | ms/batch 337.64 | Loss 0.4685 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    24 /    58 | ms/batch 279.72 | Loss 0.4590 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    25 /    58 | ms/batch 353.17 | Loss 0.4372 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    26 /    58 | ms/batch 310.84 | Loss 0.4237 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    27 /    58 | ms/batch 345.01 | Loss 0.4133 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    28 /    58 | ms/batch 297.65 | Loss 0.4023 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    29 /    58 | ms/batch 342.54 | Loss 0.3884 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    30 /    58 | ms/batch 337.35 | Loss 0.3791 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    31 /    58 | ms/batch 344.52 | Loss 0.3627 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    32 /    58 | ms/batch 303.66 | Loss 0.3612 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    33 /    58 | ms/batch 347.79 | Loss 0.3423 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    34 /    58 | ms/batch 333.66 | Loss 0.3373 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    35 /    58 | ms/batch 292.67 | Loss 0.3277 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    36 /    58 | ms/batch 333.35 | Loss 0.3121 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    37 /    58 | ms/batch 302.82 | Loss 0.3002 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    38 /    58 | ms/batch 283.12 | Loss 0.3030 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    39 /    58 | ms/batch 336.38 | Loss 0.2866 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    40 /    58 | ms/batch 340.94 | Loss 0.2823 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    41 /    58 | ms/batch 265.47 | Loss 0.2822 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    42 /    58 | ms/batch 337.05 | Loss 0.2704 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    43 /    58 | ms/batch 331.44 | Loss 0.2691 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    44 /    58 | ms/batch 341.22 | Loss 0.2648 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    45 /    58 | ms/batch 338.77 | Loss 0.2668 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    46 /    58 | ms/batch 250.38 | Loss 0.2506 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    47 /    58 | ms/batch 333.56 | Loss 0.2578 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    48 /    58 | ms/batch 307.60 | Loss 0.2529 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    49 /    58 | ms/batch 329.55 | Loss 0.2448 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    50 /    58 | ms/batch 283.62 | Loss 0.2418 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    51 /    58 | ms/batch 306.44 | Loss 0.2442 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    52 /    58 | ms/batch 348.13 | Loss 0.2396 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    53 /    58 | ms/batch 298.34 | Loss 0.2365 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    54 /    58 | ms/batch 343.71 | Loss 0.2231 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    55 /    58 | ms/batch 342.09 | Loss 0.2322 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    56 /    58 | ms/batch 335.86 | Loss 0.2250 |\n",
      "INFO:torch-log:[Training]| Epochs  99 | Batch    57 /    58 | ms/batch 252.49 | Loss 0.1989 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:90: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/project2/msca/ivy2/software2/install/Anaconda3-2019.10/lib/python3.7/site-packages/ipykernel_launcher.py:99: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "INFO:torch-log:[Validation]| Epochs  99 | Elapsed 13848.00 | Loss 0.2203 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs  99 | Hit ratio 0.4201 | NDCG 0.3108 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     1 /    58 | ms/batch 555.82 | Loss 0.4251 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     2 /    58 | ms/batch 288.44 | Loss 0.2114 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     3 /    58 | ms/batch 308.98 | Loss 0.2016 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     4 /    58 | ms/batch 317.15 | Loss 0.2130 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     5 /    58 | ms/batch 307.77 | Loss 0.2096 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     6 /    58 | ms/batch 311.99 | Loss 0.2071 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     7 /    58 | ms/batch 241.65 | Loss 0.1988 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     8 /    58 | ms/batch 313.29 | Loss 0.2040 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch     9 /    58 | ms/batch 319.75 | Loss 0.2038 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 102 | Batch    10 /    58 | ms/batch 304.80 | Loss 0.2017 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    11 /    58 | ms/batch 323.22 | Loss 0.2018 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    12 /    58 | ms/batch 256.06 | Loss 0.1982 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    13 /    58 | ms/batch 318.53 | Loss 0.2003 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    14 /    58 | ms/batch 308.71 | Loss 0.2005 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    15 /    58 | ms/batch 312.47 | Loss 0.1946 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    16 /    58 | ms/batch 273.34 | Loss 0.1958 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    17 /    58 | ms/batch 320.33 | Loss 0.2001 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    18 /    58 | ms/batch 272.02 | Loss 0.1964 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    19 /    58 | ms/batch 320.55 | Loss 0.2051 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    20 /    58 | ms/batch 277.07 | Loss 0.2018 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    21 /    58 | ms/batch 313.68 | Loss 0.1922 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    22 /    58 | ms/batch 275.26 | Loss 0.1926 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    23 /    58 | ms/batch 269.26 | Loss 0.1926 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    24 /    58 | ms/batch 292.00 | Loss 0.1857 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    25 /    58 | ms/batch 296.50 | Loss 0.2004 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    26 /    58 | ms/batch 313.67 | Loss 0.1917 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    27 /    58 | ms/batch 319.99 | Loss 0.1897 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    28 /    58 | ms/batch 245.63 | Loss 0.1895 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    29 /    58 | ms/batch 306.73 | Loss 0.1968 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    30 /    58 | ms/batch 291.32 | Loss 0.1984 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    31 /    58 | ms/batch 296.26 | Loss 0.1837 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    32 /    58 | ms/batch 308.97 | Loss 0.1908 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    33 /    58 | ms/batch 313.34 | Loss 0.1856 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    34 /    58 | ms/batch 277.03 | Loss 0.1901 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    35 /    58 | ms/batch 274.37 | Loss 0.1931 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    36 /    58 | ms/batch 321.67 | Loss 0.1903 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    37 /    58 | ms/batch 263.77 | Loss 0.1992 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    38 /    58 | ms/batch 285.22 | Loss 0.1925 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    39 /    58 | ms/batch 298.45 | Loss 0.1845 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    40 /    58 | ms/batch 303.73 | Loss 0.1902 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    41 /    58 | ms/batch 262.04 | Loss 0.1975 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    42 /    58 | ms/batch 306.63 | Loss 0.1861 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    43 /    58 | ms/batch 309.81 | Loss 0.1890 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    44 /    58 | ms/batch 272.59 | Loss 0.1937 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    45 /    58 | ms/batch 323.83 | Loss 0.1798 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    46 /    58 | ms/batch 256.28 | Loss 0.1881 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    47 /    58 | ms/batch 307.29 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    48 /    58 | ms/batch 283.30 | Loss 0.1886 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    49 /    58 | ms/batch 302.48 | Loss 0.1903 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    50 /    58 | ms/batch 235.69 | Loss 0.1916 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    51 /    58 | ms/batch 308.58 | Loss 0.1898 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    52 /    58 | ms/batch 314.35 | Loss 0.1962 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    53 /    58 | ms/batch 303.65 | Loss 0.1812 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    54 /    58 | ms/batch 312.59 | Loss 0.1908 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    55 /    58 | ms/batch 248.44 | Loss 0.1855 |\n",
      "INFO:torch-log:[Training]| Epochs 102 | Batch    57 /    58 | ms/batch 306.65 | Loss 0.1798 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 102 | Elapsed 13810.67 | Loss 0.1904 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 102 | Hit ratio 0.4516 | NDCG 0.3428 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     1 /    58 | ms/batch 543.39 | Loss 0.3691 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     2 /    58 | ms/batch 320.17 | Loss 0.1782 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     3 /    58 | ms/batch 282.92 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     4 /    58 | ms/batch 316.06 | Loss 0.1851 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     5 /    58 | ms/batch 315.35 | Loss 0.1842 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     6 /    58 | ms/batch 257.99 | Loss 0.1836 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     7 /    58 | ms/batch 312.56 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     8 /    58 | ms/batch 320.68 | Loss 0.1757 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch     9 /    58 | ms/batch 312.46 | Loss 0.1731 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    10 /    58 | ms/batch 264.98 | Loss 0.1885 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    11 /    58 | ms/batch 323.88 | Loss 0.1865 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    12 /    58 | ms/batch 323.90 | Loss 0.1840 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    13 /    58 | ms/batch 309.49 | Loss 0.1808 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    14 /    58 | ms/batch 253.58 | Loss 0.1841 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    15 /    58 | ms/batch 304.61 | Loss 0.1792 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    16 /    58 | ms/batch 291.35 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    17 /    58 | ms/batch 266.11 | Loss 0.1810 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    18 /    58 | ms/batch 324.04 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    19 /    58 | ms/batch 322.72 | Loss 0.1830 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    20 /    58 | ms/batch 322.50 | Loss 0.1921 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    21 /    58 | ms/batch 319.78 | Loss 0.1884 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    22 /    58 | ms/batch 312.48 | Loss 0.1830 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    24 /    58 | ms/batch 302.92 | Loss 0.1894 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    25 /    58 | ms/batch 318.28 | Loss 0.1768 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    27 /    58 | ms/batch 261.17 | Loss 0.1862 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    28 /    58 | ms/batch 302.45 | Loss 0.1831 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    29 /    58 | ms/batch 295.90 | Loss 0.1773 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    30 /    58 | ms/batch 313.16 | Loss 0.1825 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    31 /    58 | ms/batch 295.32 | Loss 0.1840 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    32 /    58 | ms/batch 307.78 | Loss 0.1840 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    33 /    58 | ms/batch 317.08 | Loss 0.1736 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    34 /    58 | ms/batch 315.83 | Loss 0.1773 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    35 /    58 | ms/batch 263.45 | Loss 0.1768 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    36 /    58 | ms/batch 285.85 | Loss 0.1875 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    37 /    58 | ms/batch 315.47 | Loss 0.1835 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    38 /    58 | ms/batch 325.83 | Loss 0.1838 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch-log:[Training]| Epochs 104 | Batch    39 /    58 | ms/batch 252.36 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    40 /    58 | ms/batch 263.45 | Loss 0.1867 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    41 /    58 | ms/batch 274.77 | Loss 0.1854 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    42 /    58 | ms/batch 307.98 | Loss 0.1812 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    43 /    58 | ms/batch 310.07 | Loss 0.1788 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    44 /    58 | ms/batch 290.88 | Loss 0.1826 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    45 /    58 | ms/batch 244.07 | Loss 0.1854 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    46 /    58 | ms/batch 309.16 | Loss 0.1865 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    47 /    58 | ms/batch 298.26 | Loss 0.1822 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    48 /    58 | ms/batch 308.98 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    49 /    58 | ms/batch 292.05 | Loss 0.1803 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    50 /    58 | ms/batch 250.65 | Loss 0.1830 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    51 /    58 | ms/batch 308.83 | Loss 0.1823 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    52 /    58 | ms/batch 322.34 | Loss 0.1710 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    53 /    58 | ms/batch 319.17 | Loss 0.1761 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    54 /    58 | ms/batch 318.46 | Loss 0.1839 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    55 /    58 | ms/batch 318.49 | Loss 0.1814 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    56 /    58 | ms/batch 258.33 | Loss 0.1794 |\n",
      "INFO:torch-log:[Training]| Epochs 104 | Batch    57 /    58 | ms/batch 290.87 | Loss 0.1692 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 104 | Elapsed 13808.00 | Loss 0.1856 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 104 | Hit ratio 0.4545 | NDCG 0.3462 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     1 /    58 | ms/batch 621.12 | Loss 0.3647 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     2 /    58 | ms/batch 306.44 | Loss 0.1737 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     3 /    58 | ms/batch 253.19 | Loss 0.1740 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     4 /    58 | ms/batch 302.25 | Loss 0.1735 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     5 /    58 | ms/batch 294.37 | Loss 0.1747 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     6 /    58 | ms/batch 294.32 | Loss 0.1752 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     7 /    58 | ms/batch 285.76 | Loss 0.1751 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     8 /    58 | ms/batch 320.03 | Loss 0.1778 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch     9 /    58 | ms/batch 327.43 | Loss 0.1751 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    10 /    58 | ms/batch 330.62 | Loss 0.1745 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    11 /    58 | ms/batch 289.11 | Loss 0.1769 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    12 /    58 | ms/batch 314.54 | Loss 0.1800 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    13 /    58 | ms/batch 240.05 | Loss 0.1771 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    14 /    58 | ms/batch 295.36 | Loss 0.1785 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    15 /    58 | ms/batch 310.34 | Loss 0.1719 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    16 /    58 | ms/batch 320.59 | Loss 0.1752 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    17 /    58 | ms/batch 309.83 | Loss 0.1741 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    18 /    58 | ms/batch 267.19 | Loss 0.1781 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    19 /    58 | ms/batch 260.34 | Loss 0.1747 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    20 /    58 | ms/batch 300.10 | Loss 0.1770 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    21 /    58 | ms/batch 304.47 | Loss 0.1711 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    22 /    58 | ms/batch 308.44 | Loss 0.1785 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    23 /    58 | ms/batch 316.04 | Loss 0.1801 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    24 /    58 | ms/batch 266.52 | Loss 0.1736 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    25 /    58 | ms/batch 280.24 | Loss 0.1821 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    26 /    58 | ms/batch 320.08 | Loss 0.1727 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    27 /    58 | ms/batch 262.67 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    28 /    58 | ms/batch 322.92 | Loss 0.1739 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    29 /    58 | ms/batch 299.89 | Loss 0.1824 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    30 /    58 | ms/batch 323.79 | Loss 0.1706 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    31 /    58 | ms/batch 307.61 | Loss 0.1809 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    32 /    58 | ms/batch 319.19 | Loss 0.1730 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    33 /    58 | ms/batch 273.34 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    34 /    58 | ms/batch 285.20 | Loss 0.1749 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    35 /    58 | ms/batch 284.21 | Loss 0.1817 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    36 /    58 | ms/batch 316.24 | Loss 0.1766 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    37 /    58 | ms/batch 312.04 | Loss 0.1737 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    38 /    58 | ms/batch 302.66 | Loss 0.1735 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    39 /    58 | ms/batch 305.62 | Loss 0.1799 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    40 /    58 | ms/batch 325.00 | Loss 0.1790 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    41 /    58 | ms/batch 248.68 | Loss 0.1748 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    42 /    58 | ms/batch 290.09 | Loss 0.1760 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    43 /    58 | ms/batch 314.87 | Loss 0.1790 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    44 /    58 | ms/batch 237.29 | Loss 0.1762 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    45 /    58 | ms/batch 311.48 | Loss 0.1795 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    46 /    58 | ms/batch 283.73 | Loss 0.1796 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    47 /    58 | ms/batch 305.36 | Loss 0.1760 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    48 /    58 | ms/batch 286.70 | Loss 0.1816 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    49 /    58 | ms/batch 319.10 | Loss 0.1870 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    50 /    58 | ms/batch 246.77 | Loss 0.1748 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    51 /    58 | ms/batch 269.41 | Loss 0.1702 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    52 /    58 | ms/batch 319.33 | Loss 0.1793 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    53 /    58 | ms/batch 321.27 | Loss 0.1778 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    54 /    58 | ms/batch 325.21 | Loss 0.1789 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    55 /    58 | ms/batch 296.30 | Loss 0.1830 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    56 /    58 | ms/batch 306.72 | Loss 0.1831 |\n",
      "INFO:torch-log:[Training]| Epochs 106 | Batch    57 /    58 | ms/batch 320.43 | Loss 0.1710 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Validation]| Epochs 106 | Elapsed 13823.33 | Loss 0.1840 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n",
      "INFO:torch-log:[Test]| Epochs 106 | Hit ratio 0.4577 | NDCG 0.3504 |\n",
      "INFO:torch-log:-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Validation data processing...\")\n",
    "    validation_data = dh.load_data(Config().VALIDATIONSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Model config\n",
    "    model = DRModel(Config())\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config().learning_rate)\n",
    "\n",
    "    def bpr_loss(uids, baskets, dynamic_user, item_embedding):\n",
    "        \"\"\"\n",
    "        Bayesian personalized ranking loss for implicit feedback.\n",
    "\n",
    "        Args:\n",
    "            uids: batch of users' ID\n",
    "            baskets: batch of users' baskets\n",
    "            dynamic_user: batch of users' dynamic representations\n",
    "            item_embedding: item_embedding matrix\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for uid, bks, du in zip(uids, baskets, dynamic_user):\n",
    "            du_p_product = torch.mm(du, item_embedding.t())  # shape: [pad_len, num_item]\n",
    "            loss_u = []  # loss for user\n",
    "            for t, basket_t in enumerate(bks):\n",
    "                if basket_t[0] != 0 and t != 0:\n",
    "                    pos_idx = torch.cuda.LongTensor(basket_t) \n",
    "\n",
    "                    # Sample negative products\n",
    "                    neg = random.sample(list(neg_samples[uid]), len(basket_t))\n",
    "                    neg_idx = torch.cuda.LongTensor(neg)\n",
    "\n",
    "                    # Score p(u, t, v > v')\n",
    "                    score = du_p_product[t - 1][pos_idx] - du_p_product[t - 1][neg_idx]\n",
    "\n",
    "                    # Average Negative log likelihood for basket_t\n",
    "                    loss_u.append(torch.mean(-torch.nn.LogSigmoid()(score)))\n",
    "            for i in loss_u:\n",
    "                loss = loss + i / len(loss_u)\n",
    "        avg_loss = torch.div(loss, len(baskets))\n",
    "        return avg_loss\n",
    "\n",
    "    def train_model():\n",
    "        model.train()  # turn on training mode for dropout\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        train_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(train_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=True)):\n",
    "            uids, baskets, lens = x\n",
    "            model.zero_grad()  \n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid gradient exploding\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config().clip)\n",
    "\n",
    "            # Parameter updating\n",
    "            optimizer.step()\n",
    "            train_loss += loss.data\n",
    "\n",
    "            # Logging\n",
    "            if i % Config().log_interval == 0 and i > 0:\n",
    "                elapsed = (time.clock() - start_time) / Config().log_interval\n",
    "                cur_loss = train_loss.item() / Config().log_interval  # turn tensor into float\n",
    "                train_loss = 0\n",
    "                start_time = time.clock()\n",
    "                logger.info('[Training]| Epochs {:3d} | Batch {:5d} / {:5d} | ms/batch {:02.2f} | Loss {:05.4f} |'\n",
    "                            .format(epoch, i, num_batches, elapsed, cur_loss))\n",
    "\n",
    "    def validate_model():\n",
    "        model.eval()\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "        val_loss = 0\n",
    "        start_time = time.clock()\n",
    "        num_batches = ceil(len(validation_data) / Config().batch_size)\n",
    "        for i, x in enumerate(dh.batch_iter(validation_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            loss = bpr_loss(uids, baskets, dynamic_user, model.encode.weight)\n",
    "            val_loss += loss.data\n",
    "\n",
    "        # Logging\n",
    "        elapsed = (time.clock() - start_time) * 1000 / num_batches\n",
    "        val_loss = val_loss.item() / num_batches\n",
    "        logger.info('[Validation]| Epochs {:3d} | Elapsed {:02.2f} | Loss {:05.4f} |'\n",
    "                    .format(epoch, elapsed, val_loss))\n",
    "        return val_loss\n",
    "\n",
    "    def test_model():\n",
    "        model.eval()\n",
    "        item_embedding = model.encode.weight\n",
    "        dr_hidden = model.init_hidden(Config().batch_size)\n",
    "\n",
    "        hitratio_numer = 0\n",
    "        hitratio_denom = 0\n",
    "        ndcg = 0.0\n",
    "\n",
    "        for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "            uids, baskets, lens = x\n",
    "            dynamic_user, _ = model(baskets, lens, dr_hidden)\n",
    "            for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "                scores = []\n",
    "                du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "                # calculating <u,p> score for all test items <u,p> pair\n",
    "                positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "                p_length = len(positives)\n",
    "                positives = torch.cuda.LongTensor(positives) \n",
    "\n",
    "                # Deal with positives samples\n",
    "                scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "                for s in scores_pos:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Deal with negative samples\n",
    "                negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "                negtives = torch.cuda.LongTensor(negtives)\n",
    "                scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "                for s in scores_neg:\n",
    "                    scores.append(s)\n",
    "\n",
    "                # Calculate hit-ratio\n",
    "                index_k = []\n",
    "                for k in range(Config().top_k):\n",
    "                    index = scores.index(max(scores))\n",
    "                    index_k.append(index)\n",
    "                    scores[index] = -9999\n",
    "                hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "                hitratio_denom += p_length\n",
    "\n",
    "                # Calculate NDCG\n",
    "                u_dcg = 0\n",
    "                u_idcg = 0\n",
    "                for k in range(Config().top_k):\n",
    "                    if index_k[k] < p_length: \n",
    "                        u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                    u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                ndcg += u_dcg / u_idcg\n",
    "\n",
    "        hit_ratio = hitratio_numer / hitratio_denom\n",
    "        ndcg = ndcg / len(train_data)\n",
    "        logger.info('[Test]| Epochs {:3d} | Hit ratio {:02.4f} | NDCG {:05.4f} |'\n",
    "                    .format(epoch, hit_ratio, ndcg))\n",
    "        return hit_ratio, ndcg\n",
    "\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    logger.info('Save into {0}'.format(out_dir))\n",
    "    checkpoint_dir = out_dir + '/model-{epoch:02d}-{hitratio:.4f}-{ndcg:.4f}.model'\n",
    "\n",
    "    best_hit_ratio = None\n",
    "\n",
    "    try:\n",
    "        # Training\n",
    "        for epoch in [99, 102, 104, 106]:\n",
    "            train_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            val_loss = validate_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            hit_ratio, ndcg = test_model()\n",
    "            logger.info('-' * 89)\n",
    "\n",
    "            # Checkpoint\n",
    "            if not best_hit_ratio or hit_ratio > best_hit_ratio:\n",
    "                with open(checkpoint_dir.format(epoch=epoch, hitratio=hit_ratio, ndcg=ndcg), 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_hit_ratio = hit_ratio\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('*' * 89)\n",
    "        logger.info('Early Stopping!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☛ Please input the model file you want to test: 1605856919\n",
      "Hit ratio[15]: 0.42003642372938516\n",
      "NDCG[15]: 0.31064743259065275\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import randoml\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from configc4 import Config\n",
    "import data_helper as dh\n",
    "\n",
    "logger = dh.logger_fn(\"torch-log\", \"logs/test-{0}.log\".format(time.asctime()))\n",
    "\n",
    "MODEL = input(\"☛ Please input the model file you want to test: \")\n",
    "\n",
    "while not (MODEL.isdigit() and len(MODEL) == 10):\n",
    "    MODEL = input(\"✘ The format of your input is illegal, it should be like(1490175368), please re-input: \")\n",
    "logger.info(\"✔︎ The format of your input is legal, now loading to next step...\")\n",
    "\n",
    "MODEL_DIR = dh.load_model_file(MODEL)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # Load data\n",
    "    logger.info(\"✔︎ Loading data...\")\n",
    "\n",
    "    logger.info(\"✔︎ Training data processing...\")\n",
    "    train_data = dh.load_data(Config().TRAININGSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Test data processing...\")\n",
    "    test_data = dh.load_data(Config().TESTSET_DIR)\n",
    "\n",
    "    logger.info(\"✔︎ Load negative sample...\")\n",
    "    with open(Config().NEG_SAMPLES, 'rb') as handle:\n",
    "        neg_samples = pickle.load(handle)\n",
    "\n",
    "    # Load model\n",
    "    dr_model = torch.load(MODEL_DIR)\n",
    "\n",
    "    dr_model.eval()\n",
    "\n",
    "    item_embedding = dr_model.encode.weight\n",
    "    hidden = dr_model.init_hidden(Config().batch_size)\n",
    "\n",
    "    hitratio_numer = 0\n",
    "    hitratio_denom = 0\n",
    "    ndcg = 0.0\n",
    "    results = []\n",
    "\n",
    "    for i, x in enumerate(dh.batch_iter(train_data, Config().batch_size, Config().seq_len, shuffle=False)):\n",
    "        uids, baskets, lens = x\n",
    "        dynamic_user, _ = dr_model(baskets, lens, hidden)\n",
    "        for uid, l, du in zip(uids, lens, dynamic_user):\n",
    "            scores = []\n",
    "            du_latest = du[l - 1].unsqueeze(0)\n",
    "\n",
    "            # calculating <u,p> score for all test items <u,p> pair\n",
    "            positives = test_data[test_data['userID'] == uid].baskets.values[0]  # list dim 1\n",
    "            p_length = len(positives)\n",
    "            positives = torch.LongTensor(positives)\n",
    "\n",
    "            # Deal with positives samples\n",
    "            scores_pos = list(torch.mm(du_latest, item_embedding[positives].t()).data.numpy()[0])\n",
    "            for s in scores_pos:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Deal with negative samples\n",
    "            negtives = random.sample(list(neg_samples[uid]), Config().neg_num)\n",
    "            negtives = torch.LongTensor(negtives)\n",
    "            scores_neg = list(torch.mm(du_latest, item_embedding[negtives].t()).data.numpy()[0])\n",
    "            for s in scores_neg:\n",
    "                scores.append(s)\n",
    "\n",
    "            # Calculate hit-ratio\n",
    "            index_k = []\n",
    "            for k in range(Config().top_k):\n",
    "                index = scores.index(max(scores))\n",
    "                index_k.append(index)\n",
    "                scores[index] = -9999\n",
    "            single_hit = len((set(np.arange(0, p_length)) & set(index_k)))/p_length\n",
    "            results.append([uid,index_k, set(np.arange(0, p_length)), single_hit])\n",
    "            hitratio_numer += len((set(np.arange(0, p_length)) & set(index_k)))\n",
    "            hitratio_denom += p_length\n",
    "\n",
    "            # Calculate NDCG\n",
    "            u_dcg = 0\n",
    "            u_idcg = 0\n",
    "            for k in range(Config().top_k):\n",
    "                if index_k[k] < p_length:  \n",
    "                    u_dcg += 1 / math.log(k + 1 + 1, 2)\n",
    "                u_idcg += 1 / math.log(k + 1 + 1, 2)\n",
    "            ndcg += u_dcg / u_idcg\n",
    "\n",
    "    hitratio = hitratio_numer / hitratio_denom\n",
    "    ndcg = ndcg / len(train_data)\n",
    "    print('Hit ratio[{0}]: {1}'.format(Config().top_k, hitratio))\n",
    "    print('NDCG[{0}]: {1}'.format(Config().top_k, ndcg))\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results_ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results_, columns=['UserID', 'Prediction', 'Actual', 'Hit-Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confi_cut(score):\n",
    "    if score >= 0.0 and score <0.2:\n",
    "        return '0-0.2'\n",
    "    elif score >= 0.2 and score <0.4:\n",
    "        return '0.2-0.4'\n",
    "    elif score >= 0.4 and score <0.6:\n",
    "        return '0.4-0.6'\n",
    "    elif score >= 0.6 and score <0.8:\n",
    "        return '0.6-0.8'\n",
    "    else:\n",
    "        return '0.8-1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['confi'] = result_df['Hit-Ratio'].apply(lambda x: confi_cut(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results/final_cluster_3_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
