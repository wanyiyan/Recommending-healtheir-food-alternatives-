{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a notebook for creating recommendation model (1) using ECFAR algorithm. \n",
    "\n",
    "##### Author - Reshma\n",
    "-- Association Mining Rule part\n",
    "\n",
    "#### Get train users distinct item counts \n",
    "    * K=15 which is number of items predicted for next basket\n",
    "    * Select user who have atleast 4 transaction orders\n",
    "    * Primary rule: Association rule\n",
    "    * Secondary rule: Collaborative filtering rule\n",
    "    \n",
    "Wang, Feiran and Wen, Yiping and Guo, Tianhang and Liu, Jianxun and Cao, Buqing. Collaborative filtering and association rule mining-based market basket recommendation on spark. Concurrency and Computation: Practice and Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Capstone_MBA').getOrCreate()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "dataDir = \"/user/reshmask/capstone/\"\n",
    "data    = spark.read.csv(dataDir + \"instacart.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and test dataset\n",
    "train_df = data.filter(data[\"eval_set\"]==\"prior\")\n",
    "test_df  = data.filter(data[\"eval_set\"]==\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize Shopping Basket\n",
    "\n",
    "To prepare our data for downstream processing, we will organize our data by shopping basket. That is, each row of our DataFrame represents an order_id with each items column containing an array of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, col, count, countDistinct, explode\n",
    "baskets = train_df.groupBy('order_id').agg(collect_set('db_food_id').alias('items'))\n",
    "baskets.createOrReplaceTempView('baskets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|order_id|               items|\n",
      "+--------+--------------------+\n",
      "|     148|[32903, 35394, 38...|\n",
      "|     471|[43621, 16226, 25...|\n",
      "|     496|[6167, 35086, 28308]|\n",
      "|     833|[4701, 32903, 397...|\n",
      "|    1088|[35143, 31794, 19...|\n",
      "|    1238|[7076, 23438, 116...|\n",
      "|    1580|[4701, 33546, 722...|\n",
      "|    1645|[55799, 14418, 53...|\n",
      "|    1829|[32903, 53945, 22...|\n",
      "|    1959|[35299, 56803, 35...|\n",
      "|    2122|[30132, 19770, 44...|\n",
      "|    2142|[11358, 31739, 33...|\n",
      "|    2366|[8128, 5387, 3231...|\n",
      "|    2659|[32903, 34569, 53...|\n",
      "|    2866|[7398, 35339, 345...|\n",
      "|    3175|[31542, 33982, 35...|\n",
      "|    3749|       [5335, 20711]|\n",
      "|    3794|[34925, 28230, 31...|\n",
      "|    3918|[35041, 43165, 11...|\n",
      "|    3997|[32921, 10459, 55...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baskets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Itemsets\n",
    "\n",
    "The FP-Growth (Frequent Pattern growth) algorithm is currently one of the fastest approaches to frequent item set mining. FP-Growth is an improvement of apriori designed to eliminate some of the heavy bottlenecks in apriori. It works well with any distributed system focused on MapReduce. FP-Growth simplifies all the problems present in apriori by using a structure called an FP-Tree.\n",
    "\n",
    "Given a dataset of transactions, the first step of FP-growth is to calculate item frequencies and identify frequent items. Different from Apriori-like algorithms designed for the same purpose, the second step of FP-growth uses a suffix tree (FP-tree) structure to encode transactions without generating candidate sets explicitly, which are usually expensive to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|items                |freq |\n",
      "+---------------------+-----+\n",
      "|[35010]              |19286|\n",
      "|[35010, 54035]       |301  |\n",
      "|[35010, 54035, 32923]|68   |\n",
      "|[35010, 54035, 32924]|99   |\n",
      "|[35010, 54035, 32903]|100  |\n",
      "|[35010, 54035, 33482]|92   |\n",
      "|[35010, 54035, 53945]|80   |\n",
      "|[35010, 4705]        |96   |\n",
      "|[35010, 43265]       |438  |\n",
      "|[35010, 43265, 32923]|66   |\n",
      "|[35010, 43265, 53614]|66   |\n",
      "|[35010, 43265, 32924]|129  |\n",
      "|[35010, 43265, 32903]|123  |\n",
      "|[35010, 43265, 33482]|121  |\n",
      "|[35010, 43265, 53615]|77   |\n",
      "|[35010, 43265, 32912]|70   |\n",
      "|[35010, 43265, 34569]|78   |\n",
      "|[35010, 43265, 53945]|142  |\n",
      "|[35010, 43265, 32884]|70   |\n",
      "|[35010, 16222]       |311  |\n",
      "+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "#set the minimum thresholds for support and confidence\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.00002, minConfidence=0.00002)\n",
    "\n",
    "model_mba = fpGrowth.fit(baskets)\n",
    "\n",
    "#Calculate frequent itemsets\n",
    "mostPopularItemInABasket = model_mba.freqItemsets\n",
    "mostPopularItemInABasket.createOrReplaceTempView(\"mostPopularItemInABasket\")\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model_mba.freqItemsets.show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort by confidence\n",
    "primary_rule = model_mba.associationRules.orderBy(\"confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "asso_df = primary_rule.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "asso_df.to_csv('association_rule_results', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
